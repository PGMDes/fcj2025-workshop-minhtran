[{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Event Objectives Complete large-scale migration and modernization with AWS Modernize applications using generative AI–powered tools Group discussion: Application modernization—Accelerate business transformation Transform VMware with AI-driven cloud-modernization technologies AWS security at scale: From development to production Speakers Nguyen Van Hai - Director of Software Engineering, Techcombank Nguyen The Vinh - Co-Founder \u0026amp; CTO, Ninety Eight Nguyen Minh Nganh - AI Specialist, OCB Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Key Highlights 1. Learn large-scale migration and modernization strategies with AWS through real-world case studies from Techcombank The Modernization Journey of Techcombank\u0026rsquo;s\nAssess: Inventory the environment and identify gaps. Mobilize: Establish CCoE, define guardrails, and build cloud fluency. Migrate \u0026amp; Modernize: Prioritize high-impact workloads. Reinvent: AI, automation, data products, new business models. Generative AI in Modernization.\nCode Transformation: Java 8 -\u0026gt; 21, .NET -\u0026gt; .NET 8 Dependency Mapping: Automatied analysis of system relationships. Environment Assessment: Amazon: thousands of services modernized with AI Solutions and strategies Techcombank applied when using AWS services. Modernization with AWS native technologies:\nAmazon EKS Amazon Aurora MySQL Amazon MSK Amazon ElastiCache for Redis OSS. Overview of the Modernization Strategy Blueprint.\nAlign: Executive sponsorship and business drivers | Assess: Understand people, process, and technology | Mobilize: CoE, governance, training | Modernize: Replatform, refactor, rebuild | Reinvent: Data, AI, and modern apps for innovation 2. Gain knowledge about modernizing applications using Generative Al-powered tools, with practical insights from VPBank Modernization is the process of progressively transforming applications to achieve the availability, scalability, business agility, and cost optimization benefits of running on the cloud. Top 4 Use cases – App Modernization with GenAI.\nUse case 1: Streamline VMware Migration with AWS Transform for VMware\nAccelerate infrastructure migration and modernization with intelligent discovery and automated execution\nSlash VMware migration timelines with AWS Transform\u0026rsquo;s intelligent automation.\nConvert complex network configurations in hours instead of weeks using Al-powered discovery, dependency mapping, and automated wave planning.\nScale your migration practice with automated security group creation, intelligent EC2 instance selection, and flexible deployment options including hub-and-spoke or isolated VPC configurations.\nGain up to 90% improverent in execution times while reducing manual effort by 80%.\nUse case 2: GenAI Development with AWS Serverless and Container Solutions\nBuild Enterprise-Ready GenAI Applications Across AWS Serverless and Containers Platforms\nAWS offers two powerful paths for GenAI application development and deployment:\nServerless with AWS Bedrock: Rapidly develop and deploy GenAI applications using AWS Lambda, ECS with Fargate, Step Functions, and EventBridge. Ideal for chatbots, document generation, and intelligent content processing. Leverage the latest AWS Bedrock updates and reference architectures.\nContainer-Based with Amazon EKS: Build, train, and run GenAl apps on Kubernetes, benefiting from its powerful orchestration capabilities. Utilize open-source tools and cloud-native services for scalable GenAl workloads. Flexible deployment across cloud and on-premises environments with constant innovation from the OSS community.\nChoose one approach or combine both to best fit your specific GenAI application requirements and accelerate your AI journey.\nUse case 3: Revolutionize .NET Modernization with AWS Transform for .NET\nTransform legacy Windows applications to cloud-native with AI-powered automation.\nModernize Windows-based applications up to 4× faster with AWS Transform for .NET. Leverage AI automation to analyze dependencies, refactor code, and optimize for Linux deployment while cutting licensing costs by up to 40%. Transform hundreds of applications in parallel with automated testing and validation câpbilities—from legacy MVC applications to WCF services. Advanced features include automatic UI modernization, private package handling, and intelligent wave planning, delivering comprehensive modernization with exceptional speed. Use case 4: Elevate Platform Engineering with GenAI \u0026amp; IDP\nHarness the power of intelligent assistants like AWS Transform Developer with Internal Development Platforms.\nScaling modernization at an enterprise level requires time and investment to develop Internal Development Platforms (IDP). Gartner predicts that by 2026, 80% of software engineering organizations will establish platform teams as internal providers of reusable services, components, and tools for application delivery.\nHarness the power of intelligent assistants like AWS Transform Developer with IDPs to:\nCreate workflows and automate repetitive tasks.\nLearn IDP best practices leading organizations, such as Adobe, Expedia, JPMC, and Goldman Sachs.\nUnderstand AWS container blueprints and reference architectures to deliver accelerated velocity and scaling for an enterprise scale modernization initiative.\nCommon modernization drivers\nReduce costs\nReduce/eliminate Windows \u0026amp; SQL Server licensing costs Create architectures that match cost to actual load Leverage ARM64 architectures for better price-performance Increase pace of innovation\nDecompose monoliths into smaller services / microservices Take advantage of new technologies and C# language features Automate manual processes Improve ability to scale\nScale individual components / services Granular scaling with containers / serverless Attract and retain talent\n3. Gain insights from top industry experts through panel discussions on application modernization .NET Framework vs cross-platform .NET\n.NET Framework:\nWindows-only OS Version 1.0 released in 2002 Final version is 4.8*, released in 2019 Monolithic installation—A large number of libraries are installed at once. EC2, Elastic Beanstalk, ECS, and EKS. .NET (formerly .NET Core)\nCross-platform (Windows, Linux, macOS) Version 1.0 released in 2016 Current GA version is 8.0, released in 2023 Supports side-by-side installations Most libraries are distributed individually EC2, Elastic Beanstalk, ECS, EKS, Lambda Fargate AWS Transform: Orchestrated intelligence\nUnified web experience -\u0026gt; End-to-end automation -\u0026gt; Dedicated agency -\u0026gt; Goal-driven -\u0026gt; Human-in-the-loop -\u0026gt; Simplified collaboration AWS Transform for .NET\nCustomer benefits\nReduce operating costs by up to 40% Eliminate operating system license commercial costs Access a larger developer pool Cloud scale and performance. Technical benefits\nVulnerability remediation support Cross-platform support: Windows, macOS, Linux (x86-64, arm64) Compatibility with x86-64 and arm64 LightweightContainer Lambda serverless architecture Complete language upgrades in minutes via Amazon Q\nAccelerate application modernization Automatic Language Upgrade (Java, .NET) Reduce Technical Debt Save Costs and Improve Operational Efficiency Enhance Competitive Advantage Kiro application: Specification-driven development solution\nKiro helps developers and engineering teams ship high-quality software with AI agents. Kiro turns your prompts into clear requirements, system designs, and discrete tasks. Iterate with Kiro on your specifications and architecture. Kiro agents implement the specification while keeping you in control. Agent hook\nDelegate tasks to AI agents triggered by events such as ‘file save’ Agents auto-execute in the background based on your predefined prompts Agent hooks help you scale work by generating documentation, unit tests, or code performance optimizations Advanced context management\nConnect to documents, databases, APIs, and more with native MCP integrations Configure how you want Kiro agents to interact with each project via directive files Drop in an image of your UI design or a photo from your architecture discussion and Kiro can use it to guide implementation Power, flexibility, and security\nCompatible with VS Code\nKiro supports VS Code Open VSX plugins, themes, and settings in a streamlined AI-ready environment Advanced Claude models\nChoose between Claude Sonnet 3.7 or Sonnet 4, with more options coming soon Enterprise-grade security\nKiro is built and operated by AWS Use cases\nBuild new applications\nQuickly go from prototype to production code and deployment, with best practices baked in, including structured design, runbooks, or test coverage scope Build on existing applications\nWith smart specification and context management, Kiro makes it easy to integrate and extend existing applications while maintaining consistency Refactor and modernize\nKiro understands your codebase and can precisely guide refactoring across codebases exceeding one million LOC 4. Learn about AI-driven cloud modernization tailored for VMware environments The future state of your VMware workloads\nRELOCATE: Amazon EVS | REHOST: Amazon EC2 | REPLATFORM TO CONTAINERS: Amazon ECS or Amazon EKS | REPLATFORM TO MANAGED SERVICES: Amazon RDS, Amazon FSx, Amazon WorkSpaces, and more | REFACTOR: Modern Application =\u0026gt; Rapid adoption, foundational cloud benefits and quick ROI....................----\u0026gt;....................All native cloud benefits and high ROI AWS Transform for VMware\nModernize VMware workloads onto Amazon EC2 with purpose-built AI agents Automate and simplify transformation tasks Reduce costs and licensing fees with Amazon EC2 Enhance security, scalability, and resilience Drive innovation with over 200 AWS native services Mapping VMware native technologies to AWS\nAn agentic AI-based approach to VMware modernization\n1. Connect to your VMware environment | 2. Analyze workloads, dependencies, and readiness | 3. Transform VMware network configurations into AWS native constructs | 4. Generate intelligent wave plans based on application dependencies | 5. Validate with your team, then execute =\u0026gt; Step-by-step transformation with human-in-the-loop validation Why AWS Transform for migrating from VMware?\nLower costs\nEliminate VMware licensing fees Optimize infrastructure costs with AI-driven instance right-sizing Faster migration\nAccelerate network transformation up to 80× Minimize disruption, preserve application integrity, and speed up the transition Improved security\nStrengthen security with a cloud-native foundation Migrate safely with a human-in-the-loop validation process Innovation at scale\nReduce technical debt and build modern, scalable applications Seamlessly integrate with over 200 AWS native services like data lakes, advanced analytics, and AI/ML 5. Connect and learn directly from AWS Solutions Architects and industry experts In this part, experts presented the challenges faced in the early stages of full-system modernization from on-premises to AWS.\nThey proposed specific plans and strategies for each area, executing the most critical parts first. They also adhered to current regulations and laws in management and did not collect user information. Once on AWS, the most important factor is the ability to scale rapidly, which leads to substantial gains when moving to the AWS environment. Applying AI is proving highly effective in their businesses—for example, anh Vinh has applied AI to detect potentially fraudulent transactions and defend against hackers in Blockchain. 6. Understand AWS security best practices from development to production environments What I Learned 5-step framework: Align → Assess → Mobilize → Modernize → Reinvent. GenAI-assisted modernization: code transformation (Java 8→21, .NET→8), dependency mapping, environment assessment. Prioritize high-impact workloads, human-in-the-loop, measure ROI. Design Thinking Problem→Pilot→Scale; prioritize value-first. Strangler Fig refactor by parts; event-driven mindset. Platform thinking/IDP, security-by-design, early governance. Technical Architecture Microservices, containers (EKS/ECS/Fargate), serverless (Lambda/Step Functions/EventBridge). Data: Aurora MySQL, MSK (Kafka), ElastiCache (Redis). VMware→AWS: rehost EC2 → replatform containers/managed → refactor app. Multi-arch (x86_64 + ARM64), end-to-end observability. Modernization Strategies Assess/Mobilize/MM/Reinvent (Techcombank blueprint). AWS Transform: for VMware \u0026amp; .NET (automated migration/testing/UI modernization). Cost-first: drop Windows/SQL licenses, right-size, ARM64. Scale \u0026amp; Innovate: decompose monolith, CI/CD automation, adopt GenAI. Application to Work Build a migration backlog by ROI; select a small pilot. Standardize container baseline (EKS) + Bedrock pattern for GenAI. Use Amazon Q/Transform to upgrade languages \u0026amp; refactor quickly. Design an internal IDP: service templates, golden paths, policy guardrails. Experience at the event “GenAI-powered Migration \u0026amp; Modernization provided a comprehensive view of transforming applications \u0026amp; databases at enterprise scale. Highlights: demos of automated VMware/.NET migration, serverless–container reference architectures, quantitative ROI lessons and battle-tested governance models, along with case studies that significantly shortened migration time and reduced costs.”\nLearning from highly specialized speakers Techcombank: operate with CCoE, measure business outcomes, 5-step roadmap. Ninety Eight: AI anti-fraud, strong security posture, real-time. OCB/LPBankS: data products, automation, safe cloud scale. Hands-on technical experience Clear view of dependency mapping, wave planning, auto SG/EC2 sizing, hub-and-spoke VPC. Auto code upgrade, cross-platform .NET, automatic UI modernization. Modern tool adoption AWS Transform (VMware/.NET), Amazon Q (auto language upgrade). Bedrock, Lambda, ECS/Fargate, EKS, Step Functions, EventBridge. Aurora, MSK, ElastiCache, EC2; IDP tooling; Kiro (spec→tasks/agents, MCP). Networking and exchange Finalize best practices from SAs \u0026amp; major banks; governance/security checklists. Connect for mentorship, pattern reuse, ROI comparison \u0026amp; benchmarking. The event created opportunities to interact directly with experts, peers, and business teams, helping to strengthen the ubiquitous language between business and tech. Lessons learned Applying DDD and event-driven patterns reduces coupling and increases scalability and resilience.\nA modernization strategy needs a phased approach and ROI measurement; avoid rushing a full system overhaul.\nAI tools like Amazon Q Developer can boost productivity when integrated into the current development workflow.\nModernize with strategy: measure ROI, prioritize by value.\nAutomation + GenAI shortens timelines and reduces technical debt.\nPlatform/IDP is a scale lever; security-by-default is indispensable.\nHuman-in-the-loop ensures safety for broad automation.\nOver 400 passionate technology developers in Ho Chi Minh City gathered at the AWS office (36th Floor) to watch the live plenary session from Ha Noi, sharing the excitement and knowledge about AWS Cloud Day Vietnam 2025\nOverall, the event not only provided technical knowledge but also changed my mindset about application design, system modernization, and more effective cross-team collaboration.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"New capabilities in Amazon SageMaker AI continue to transform how organizations develop AI models By Ankur Mehrotra | July 10, 2025 | related: Amazon Bedrock, Amazon SageMaker AI, Amazon SageMaker HyperPod, Amazon SageMaker JumpStart, Announcements\nAs AI models become more complex and specialized, the ability to train and customize models quickly can mean the difference between leading and falling behind. That’s why hundreds of thousands of customers worldwide rely on Amazon SageMaker AI — a platform that provides robust infrastructure, comprehensive tools, and managed workflows to scale and accelerate model development. Since its 2017 launch, SageMaker AI has reshaped how organizations build models by reducing complexity and optimizing performance and scalability. AWS continues to innovate — adding over 420 features since launch — delivering advanced tools to build, train, and deploy AI models faster, more flexibly, and efficiently. Today we introduce the latest SageMaker AI enhancements designed to speed up model development and training, helping organizations shorten innovation cycles and unlock AI’s potential.\nAmazon SageMaker HyperPod: Infrastructure built for large-scale model development AWS launched Amazon SageMaker HyperPod in 2023 to reduce complexity and maximize performance and efficiency when building AI models. HyperPod lets you scale generative AI development across thousands of accelerators, cutting training and fine-tuning costs for foundation models (FMs) by up to ~40%. Many leading models today are trained on HyperPod, including models from Hugging Face, Luma AI, Perplexity AI, Salesforce, Thomson Reuters, Writer, and Amazon. Training Amazon’s Nova FMs on HyperPod saved months of work and increased compute utilization above 90%.\nAmazon Trains Nova Foundation Models at Scale with SageMaker HyperPod | Amazon Web Services\nTo streamline workflows and accelerate model development and deployment, a new unified CLI and SDK provide a consistent interface that simplifies infrastructure management, unifies job submission for training and inference, and supports both recipe-driven and custom workflows with integrated monitoring and control. Today we’re adding two HyperPod capabilities to further reduce training cost and speed model development.\nCut troubleshooting time from days to minutes with HyperPod observability To rapidly bring AI innovations to market, teams need end-to-end observability over model development jobs and compute resources to optimize training performance and detect and fix bottlenecks early. For example, when investigating whether a training or fine-tuning job failed due to hardware, data scientists and ML engineers want to quickly filter and view metrics for the specific GPUs that ran the job, rather than manually scanning cluster hardware to correlate job failures with hardware faults.\nOne-click observability in SageMaker HyperPod changes how you monitor and optimize model workloads. Through a preconfigured unified dashboard in Amazon Managed Grafana with metrics automatically sent to an Amazon Managed Service for Prometheus workspace, you can view generative AI job performance, resource utilization, and cluster health in a single pane. Teams can quickly spot bottlenecks, prevent costly delays, and optimize compute. You can define automatic alerts, select metrics and events per use case, and surface them in the unified dashboard with a few clicks.\nBy cutting mean time to resolution from days to minutes, this capability speeds production timelines and maximizes ROI on AI investments.\nDatologyAI builds tools to automatically select the best data for training deep learning models.\n“We’re excited to use HyperPod’s one-click observability. Our execs need deep visibility into GPU usage. The prebuilt Grafana dashboards give exactly what we need — immediate metrics from per-job GPU usage to FSx for Lustre performance — without operating monitoring infrastructure. As a fan of Prometheus Query Language, I like being able to write queries and analyze custom metrics without worrying about the underlying infra.”\n— Josh Wills, Engineering Team, DatologyAI\nArticul8 helps enterprises build sophisticated generative AI applications.\n“With HyperPod observability, we can deploy data collection and visualization in one click, saving days of manual setup and improving cluster insights. Data scientists can quickly monitor task performance metrics like latency and identify hardware issues without manual configuration. HyperPod observability streamlines foundation model development so we can focus on delivering reliable, accessible AI innovation to customers.”\n— Renato Nascimento, CTO, Articul8\nDeploy SageMaker JumpStart models on HyperPod for fast, scalable inference After developing generative models on HyperPod, many customers import them into Amazon Bedrock, a fully managed service for building and scaling generative AI applications. Some customers prefer to use HyperPod compute to speed evaluation and productionization.\nYou can now deploy open-weight models from Amazon SageMaker JumpStart, as well as customized fine-tuned models, on HyperPod within minutes without manual infra setup. Data scientists can run inference on JumpStart models with one click, simplifying and accelerating model evaluation. The one-shot provisioning reduces manual setup, delivering reliable, scalable inference with minimal effort. Large model download times shrink from hours to minutes, speeding deployment and time to market.\nH.AI exists to push the limits of ultra-intelligent, task-oriented AI.\n“With HyperPod, we use high-performance compute to build and deploy the foundation models behind our agentic AI platform. Seamless transition from training to inference has streamlined our workflow, shortened time to production, and delivered stable performance in real environments. HyperPod helps us move from experimentation to real impact faster and more efficiently.”\n— Laurent Sifre, Co-founder \u0026amp; CTO, H.AI\nSeamless access to SageMaker AI compute from local development environments Many customers use fully managed IDEs in SageMaker AI (JupyterLab, Code Editor based on Code-OSS, and RStudio) for model development. While these managed IDEs provide secure, efficient setups, some developers prefer local IDEs for deeper debugging and customization. Previously, local IDE users like Visual Studio Code couldn’t easily run model development tasks on SageMaker AI.\nWith new remote connections to SageMaker AI, developers and data scientists can smoothly connect to SageMaker AI from local VS Code, keeping custom tools and familiar workflows while offloading execution to SageMaker AI. Developers can build and train models in their preferred IDE while SageMaker AI manages remote execution, giving them the benefits of SageMaker performance, scalability, and security. Now you can choose the IDE you prefer — managed cloud IDE or VS Code — to accelerate model development using SageMaker AI’s scalable compute and seamless integrations.\nCyberArk leads in Identity Security, offering a holistic approach centered on privileged access control.\n“With SageMaker AI remote connections, our data scientists have the flexibility to choose the IDE that makes them most productive. Teams can leverage custom local setups while accessing SageMaker AI’s infrastructure and security controls. For a security-first company, that’s crucial: it ensures sensitive data stays protected while enabling secure collaboration and higher productivity.”\n— Nir Feldman, Senior Vice President of Engineering, CyberArk\nBuild models and generative AI applications faster with fully managed MLflow 3.0 As customers across industries accelerate generative AI development, they need experiment tracking, behavior observability, and model/application performance evaluation. Customers like Cisco, SonRai, and Xometry use managed MLflow on SageMaker AI to manage ML experiments at scale. Introducing fully managed MLflow 3.0 on SageMaker AI enables experiment tracking, training monitoring, and deeper insights into model and application behavior within a single managed tool, helping accelerate generative AI development.\nConclusion This post shared several SageMaker AI innovations to speed how you build and train AI models.\nTo learn more about the new features, SageMaker AI, and customer use cases, see these resources:\nAccelerate foundation model development with one-click observability in Amazon SageMaker HyperPod Supercharge your AI workflows by connecting to SageMaker Studio from Visual Studio Code Accelerating generative AI development with fully managed MLflow 3.0 on Amazon SageMaker AI Amazon SageMaker HyperPod launches model deployments to accelerate the generative AI model development lifecycle Amazon SageMaker AI Amazon SageMaker AI customers About the author Ankur Mehrotra joined Amazon in 2008 and is the General Manager of Amazon SageMaker AI. Before SageMaker AI, he helped build Amazon.com’s advertising systems and automated pricing technologies.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Improving Weather Forecast Accuracy with GPU-Accelerated Amazon AppStream 2.0 Instances By Rayette Toles-Abdullah, Austin Park, and Chris Quarcoo | May 13, 2025 | related: Amazon AppStream 2.0, Best Practices, Customer Solutions, Public Sector\nAccess to applications and data from anywhere and on any device is increasingly important for distributed, remote, and connectivity-constrained workforces. Field researchers and meteorologists around the world rely on real-time data ingestion for critical weather capabilities.\nThe Advanced Weather Interactive Processing System (AWIPS) is a weather forecasting and visualization system used by the U.S. National Weather Service (NWS) and meteorological agencies worldwide. AWIPS enables meteorologists to monitor, analyze, and forecast weather patterns with high accuracy, producing better forecasts and more effective warnings to help protect life and property. AWIPS ingests data from multiple sources—satellites, radar, and sounding balloons. The Common AWIPS Visualization Environment (CAVE) is the client application forecasters use to interact with AWIPS. CAVE receives processed data from AWIPS servers and provides visualization tools for forecasting, issuing alerts, and exploring weather data.\nThis post shows how to stream the Common AWIPS Visualization Environment (CAVE) using Amazon AppStream 2.0 — a managed application streaming service that lets you stream desktop applications from AWS to any device without additional local hardware or software.\nBenefits of running GPU‑intensive applications on Amazon AppStream 2.0 AppStream 2.0 offers several advantages:\nUsers can access desktop applications from any supported device, including laptops, desktops, tablets, and phones—providing mobility and flexibility to remain productive whether in the office, at home, or on the move. You can scale GPU capacity up and down on demand, optimizing costs by paying only for what you use and avoiding the expense of idle hardware. There’s no need to overprovision GPU capacity, which is valuable for organizations with variable or seasonal workloads. Users can access GPU‑accelerated applications from any device, including mobile devices, delivering consistent performance regardless of the end device’s local capabilities. AppStream 2.0 handles GPU driver management, GPU fleet orchestration, and GPU‑optimized application stacks, reducing IT operational burden so teams can focus on strategic initiatives. GPU usage in AppStream 2.0 is pay-as-you-go, eliminating high upfront capital expenditure for physical GPU servers and lowering the barrier for organizations to adopt GPU workloads. AppStream 2.0 provides instance types optimized for graphics workloads such as Graphics G4dn and Graphics G5 powered by NVIDIA GPUs, enabling high performance for 3D modeling, video editing, and similar tasks. GPU workloads and data never reside on the user’s device; they remain isolated in the AWS Cloud, improving security and compliance for sensitive applications and providing centralized control over data access and IP protection. Figure 1. AWIPS CAVE visualization of Hurricane Lee on 2023-09-13 using Amazon AppStream 2.0.\nExplore the AWIPS solution The AWIPS CAVE workstation is the primary interface forecasters use. AWIPS CAVE system requirements include:\nOpenGL 2.0 compatible device Minimum 4 GB RAM Minimum 2 GB disk space for cache NVIDIA graphics card Latest NVIDIA drivers To configure AWIPS for Amazon AppStream 2.0, the steps include:\nCreate a Linux-based image Prepare a Linux environment and install AWIPS CAVE and its dependencies. This becomes the base for streaming the application.\nCreate an application optimization manifest AppStream 2.0 uses manifest files to optimize streaming performance by preloading required files. This script identifies all runtime files CAVE needs.\nAdd the application to the AppStream catalog Register CAVE in the AppStream application catalog, specifying launch path, display name, and the optimization manifest you created.\nConfigure the fleet Create a fleet to manage the instances that will run your streamed application. Choose between on-demand (cost-efficient) or always-on (instant availability).\nRecommended: Graphics G4dn (stream.graphics.g4dn.xlarge) or Graphics Pro (stream.graphics-pro.4xlarge) Choose an On‑demand fleet Set desired capacity limits Create the image Build the final Amazon AppStream image that will be used to launch streaming sessions. This captures installed applications and configuration.\nConfigure user authentication Choose between managing users directly in AppStream or integrating with your existing identity provider.\nConfigure an Amazon Cognito user pool Integrate AppStream with AWS Identity Center The diagram below shows a high-level architecture of the solution.\nFigure 2. High-level architecture showing Amazon AppStream 2.0, Amazon S3, Amazon EFS, Amazon Aurora, and Amazon EC2.\nAWIPS use cases beyond the field NWS River Forecast Centers (RFCs) use AWIPS CAVE to monitor and forecast river stages, issue flood warnings, and provide hydrology information and drought conditions. Weather Forecast Offices (WFOs) rely on AWIPS CAVE for local forecasts, alerts, and advisories, coordinating with other WFOs, federal/state agencies, and private partners. The NWS National Hurricane Center (NHC) uses AWIPS CAVE for tropical cyclone monitoring and forecasting, supporting timely public information for emergency managers and stakeholders. Universities and research institutes also use AWIPS CAVE for research, teaching, and training in meteorology and atmospheric sciences.\nConclusion Integrating AWIPS CAVE with Amazon AppStream 2.0 is a significant step forward for accessibility and forecasting capability. This solution addresses growing demand for remote access to mission‑critical meteorological applications, while delivering flexibility, cost efficiency, and performance.\nBy leveraging cloud compute and GPU acceleration, meteorologists and researchers can access advanced weather analysis tools from anywhere on any device. This improves forecast accuracy and timeliness and enhances overall service effectiveness.\nAs weather patterns grow more complex and climate challenges continue, innovations like this play a key role in supporting forecasters and improving community safety and preparedness.\nAbout the authors Rayette Toles-Abdullah — Principal Solutions Architect, Worldwide Public Sector Federal Civilian, AWS. Rayette has 23+ years of experience in systems integration, application modernization, and delivering high‑impact solutions to meet mission and business needs. Austin Park — Solutions Architect, Worldwide Public Sector Federal Civilian, AWS. Austin has 2+ years of experience, specializing in storage systems and helping customers migrate to the cloud. Chris Quarcoo — Solutions Architect, Worldwide Public Sector, AWS. Chris has 4+ years of industry experience focusing on cloud operations and container technologies, designing and deploying cloud solutions for government and public sector organizations. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Geospatial Data Lake with Amazon Redshift By Jeremy Spell and Jeff DeMuth | July 10, 2025 | Related to Amazon Redshift, AWS Glue, AWS Lake Formation, Technical How-to\nData lake architecture helps organizations move data away from expensive storage systems while maintaining query and analysis capabilities. This approach is useful for geospatial data, where builders can have terabytes of infrequently accessed data in databases that need to maintain cost efficiency. However, this requires data lake query tools that support data types and functions of geographic information systems (GIS).\nAmazon Redshift supports geospatial data queries, including GEOMETRY and GEOGRAPHY types and functions used in GIS system queries. Additionally, Amazon Redshift allows you to query geospatial data both in data lakes on Amazon S3 and in Redshift data warehouses, giving you flexibility in data access methods. Moreover, AWS Lake Formation and AWS Identity and Access Management (IAM) support in Esri\u0026rsquo;s ArcGIS Pro provide a secure way to connect between geospatial data lakes and map visualization tools. You can set up, manage, and secure geospatial data lakes in the cloud with just a few clicks.\nIn this article, we guide you through setting up a geospatial data lake using Lake Formation and querying data with ArcGIS Pro via Amazon Redshift Serverless.\nSolution Overview In the example, a county health department uses Lake Formation to protect a data lake containing Protected Health Information (PHI) data. Epidemiologists want to create maps of clinics that provide vaccinations to the community. The county\u0026rsquo;s GIS analysts need data lake access to create required maps but should not be allowed to access PHI data.\nThe solution uses Lake Formation tags to enable column-level access permissions in the database for public information including clinic name, address, zip code, and latitude/longitude coordinates, while denying access to PHI data in the same table. We use Redshift Serverless and Amazon Redshift Spectrum to access this data from ArcGIS Pro, Esri\u0026rsquo;s GIS mapping software, an AWS partner.\nBelow is the sample schema for this article.\nDescription Column Name Geoproperty Tag Patient ID patient_id No Clinic ID clinic_id Yes Clinic Address clinic_address Yes Clinic ZIP Code clinic_zip Yes Clinic City clinic_city Yes Patient First Name first_name No Patient Last Name last_name No Patient Address patient_address No Patient ZIP Code patient_zip No Vaccination Type vaccination_type No Clinic Latitude clinic_lat Yes Clinic Longitude clinic_long Yes Solution setup steps:\nDeploy infrastructure using AWS CloudFormation. Upload a CSV with sample data to an Amazon S3 bucket and run an AWS Glue crawler to discover data. Set up Lake Formation permissions. Configure Amazon Redshift Query Editor v2. Set up schema in Amazon Redshift. Create a view in Amazon Redshift. Create a local database user in ArcGIS Pro. Connect ArcGIS Pro with Redshift database. Prerequisites You need:\nAWS Account Lake Formation enabled in target AWS Region Understanding of Lake Formation and table permission setup ArcGIS Pro Network connectivity from ArcGIS Pro client to VPC where Amazon Redshift resources are deployed via VPN or AWS Direct Connect Set Up Infrastructure with AWS CloudFormation To create the demo environment, follow these steps:\nSign in to the AWS Management Console with an AWS administrator account and Lake Formation data lake administrator account—this account needs to be both account admin and data lake admin for the template to complete. Open the AWS CloudFormation console Select Launch Stack. The CloudFormation template creates the following components:\nS3 bucket – samp-clinic-db-{ACCOUNT\\_ID} AWS Glue database – samp-clinical-glue-db AWS Glue crawler – samp-glue-crawler Redshift Serverless workgroup – samp-clinical-rs-wg Redshift Serverless namespace – samp-clinical-rs-ns IAM role for Amazon Redshift – demo-RedshiftIAMRole-{UNIQUE\\_ID} IAM role for AWS Glue – samp-clinical-glue-role Lake Formation tag – geoproperty Upload CSV to S3 and Run AWS Glue Crawler Next, create a data lake in the demo environment, then use AWS Glue crawler to populate the AWS Glue database and update schema and metadata in the AWS Glue Data Catalog.\nThe CloudFormation stack already created S3 bucket, AWS Glue database, and crawler. We provide a fictional sample dataset representing patient and clinic information. Download the file and follow these steps:\nOn the AWS CloudFormation console, open the stack you just launched. On the Resources tab, select the link to the S3 bucket. Select Upload and upload the CSV file (data-with-geocode.csv), then select Upload. On the AWS Glue console, select Crawlers in the navigation. Select the crawler you created with the stack and click Run. The crawler runs for about one minute and populates a table named clinic-sample-s3\\_ACCOUNT\\_ID with sample data. Select Tables in the navigation and open the table created by the crawler. You\u0026rsquo;ll see the dataset contains PHI and personally identifiable information (PII) fields.\nNow we have a database and Data Catalog populated with schema and metadata for the rest of the process.\nSet Up Lake Formation Permissions Next, we demonstrate how to protect PHI data for compliance while still supporting GIS analysts to work effectively. To protect the data lake, use AWS Lake Formation. To set up Lake Formation permissions correctly, you need to understand how data lake access is configured.\nThe Data Catalog provides metadata and schema that allow services to access data in the lake. To access the data lake from ArcGIS Pro, use the ArcGIS Pro Redshift connector, which allows connections from ArcGIS Pro to Amazon Redshift. Amazon Redshift can access the Data Catalog and provide connections to the data lake. The CloudFormation template already created a Redshift Serverless instance and namespace with an IAM role to configure this connection. We still need to set up Lake Formation permissions so GIS analysts only access public fields, not fields containing PHI or PII. We\u0026rsquo;ll assign Lake Formation tags to columns containing public information and assign permissions to GIS analysts to access columns with these tags.\nBy default, Lake Formation is configured with Super access to IAMAllowedPrinciples; this is for backward compatibility as noted in changing default settings for data lake. To demonstrate more secure configuration, we\u0026rsquo;ll remove this default setting.\nOn the Lake Formation console, select Administration in the navigation. Under Data Catalog settings, ensure Use only IAM access control for new databases and Use only IAM access control for new tables in new databases are not selected. In the navigation, under Permissions, select Data permissions. Select IAMAllowedPrincipals and click Revoke. Select Tables in the navigation. Open table clinic-sample-s3\\_ACCOUNT\\_ID and select Edit schema. Select fields starting with clinic_ and select Edit LF-Tags. The CloudFormation stack created a Lake Formation tag named geoproperty. Assign geoproperty as key and true as value for all clinic_ fields, then select Save. Next, grant permissions to the Amazon Redshift IAM role to access fields tagged with geoproperty = true. Select Data lake permissions, then select Grant. For IAM role, select demo-RedshiftIAMRole-UNIQUE_ID. Select geoproperty for key and true for value. In Database permissions, select Describe, and in Table permissions, select Select and Describe. Configure Amazon Redshift Query Editor v2 Next, configure the initial Amazon Redshift setup necessary for database operations. We use an AWS Secrets Manager secret created by the template to manage passwords securely following AWS best practices.\nOn the Amazon Redshift console, select Query editor v2. On first launch, Amazon Redshift will show one-time configuration for the account. For this article, keep defaults and select Configure account. See additional options at Configuring your AWS account.\nThe query editor needs credentials to connect to the serverless instance; this information was created by the template and stored in Secrets Manager.\nSelect Other ways to connect, then select AWS Secrets Manager. Under Secret, select (Redshift-admin-credentials). Select Save. Set Up Schema in Amazon Redshift External schema in Amazon Redshift is a feature that references schema existing at external data sources. See External schemas in Amazon Redshift Spectrum for creating external schema. We use external schema to provide access to the data lake within Amazon Redshift. From ArcGIS Pro, we\u0026rsquo;ll connect to Amazon Redshift to access geospatial data.\nThe IAM role used when creating external schema must be associated with the Redshift namespace. The template already set this up, but you should verify before proceeding.\nIn the Redshift Serverless console, select Namespace configuration in the navigation pane. Select namespace (sample-rs-namespace). On the Security and encryption tab, you\u0026rsquo;ll see the IAM role created by CloudFormation. If the role or namespace doesn\u0026rsquo;t exist, check the stack in AWS CloudFormation before proceeding. Copy the ARN of the role for later use. Select Query data to return to the query editor. In the query editor, enter the following SQL command; remember to replace the example ARN with yours. This SQL command creates an external schema using the same Redshift role attached to the namespace to link to the AWS Glue database. CREATE EXTERNAL SCHEMA samp_clinic_sch_ext FROM DATA CATALOG database \u0026#39;sample-glue-database\u0026#39; IAM_ROLE \u0026#39;arn:aws:iam::{ACCOUNT_ID}:role/demo-RedshiftIAMRole-{UNIQUE_ID}\u0026#39;; In the query editor, execute a select query on sample-glue-database\nSELECT * FROM \u0026#34;dev\u0026#34;.\u0026#34;samp_clinic_sch_ext\u0026#34;.\u0026#34;clinic-sample_s3_{ACCOUNT_ID}\u0026#34;; Because the associated role has been granted access to columns tagged with geoproperty = true, only these fields will be returned, as shown in the following image (sample data is fictional). Use the following command to create a local schema in Amazon Redshift. External schema cannot be updated; we\u0026rsquo;ll use local schema to add geometry fields using Redshift functions.\nCREATE SCHEMA samp_clinic_sch_local Create a View in Amazon Redshift For data to display from ArcGIS Pro, we need to create a view. After schema setup, we create a view that can be accessed from ArcGIS Pro.\nAmazon Redshift provides many geospatial functions used to create views with fields that ArcGIS Pro uses to add points to maps. We\u0026rsquo;ll use one function since the dataset has latitude and longitude.\nUse the following SQL command in Amazon Redshift Query Editor to create a view named clinic_location_view. Replace {ACCOUNT_ID} with your account ID.\nCREATE OR REPLACE VIEW \u0026#34;samp_clinic_sch_local\u0026#34;.\u0026#34;clinic_location_view\u0026#34; AS SELECT clinic_id as id, clinic_lat as lat, clinic_long as long, ST_MAKEPOINT(long, lat) as geom FROM \u0026#34;dev\u0026#34;.\u0026#34;samp_clinic_sch_ext\u0026#34;.\u0026#34;clinic-sample_s3_{ACCOUNT_ID}\u0026#34; WITH NO SCHEMA BINDING; The new view in the local schema will have a geom column containing map points used by ArcGIS Pro to add points when creating maps. The points in this example are clinic locations providing vaccinations. In practice, when new clinics are built and new data is added to the data lake, their locations will appear on maps using this data.\nCreate Local Database User for ArcGIS Pro In the demo, we use a database user and group to grant permissions to the ArcGIS Pro client. Enter the following SQL in Amazon Redshift Query Editor to create the user and group:\nCREATE USER dbuser with PASSWORD \u0026#39;SET_PASSWORD_HERE\u0026#39;; CREATE GROUP esri_developer_group; ALTER GROUP esri_developer_group ADD USER dbuser; After running this, use the following commands to grant permissions to the group:\nGRANT USAGE ON SCHEMA samp_clinic_sch_local TO GROUP esri_developer_group; ALTER DEFAULT PRIVILEGES IN SCHEMA samp_clinic_sch_local GRANT SELECT ON TABLES TO GROUP esri_developer_group; GRANT SELECT ON ALL TABLES IN SCHEMA samp_clinic_sch_local TO GROUP esri_developer_group; Connect ArcGIS Pro with Redshift Database To add database connections to ArcGIS Pro, you need the endpoint for your Redshift Serverless workgroup. You can view endpoint information on the detail page of workgroup sample-rs-wg on the Redshift Serverless console. The Redshift namespace and workgroup are listed by default, as shown in the image below.\nYou can copy the endpoint in the General information section. This endpoint needs to be modified; the :5439/dev portion needs to be removed when configuring the connector in ArcGIS Pro.\nOpen ArcGIS Pro with the project where you want to add the Redshift connection. Ensure you\u0026rsquo;ve installed the Amazon Redshift ODBC connector; this is required to connect. On the menu, select Insert then Connections, Database, and New Database Connection. Under Database Platform, select Amazon Redshift. Under Server, paste the copied endpoint (remove everything after .com from the endpoint). Under Database, select your database. If your ArcGIS Pro client cannot access the endpoint, you\u0026rsquo;ll get an error at this step. A network path must exist between the ArcGIS Pro client and the Redshift Serverless endpoint. You can establish the network path with Direct Connect, AWS Site-to-Site VPN, or AWS Client VPN. While not recommended for security reasons, you can also configure a public endpoint for Amazon Redshift. Consult your network security team for best practices before allowing public access to Redshift Serverless.\nIf you have network connectivity but still get connection errors, check the security group to allow inbound from the ArcGIS Pro subnet over the port Redshift Serverless uses. The default is 5439, but port ranges can be configured depending on your environment; see Connecting to Amazon Redshift Serverless for more information.\nIf the connection is successful, ArcGIS Pro will add the Amazon Redshift connection under Connection File Name.\nSelect OK. Select the connection to display the created view with geometry (clinic_location_view). Right-click the view and select Add To Current Map. ArcGIS Pro will add the points from the view to the map. The final map was edited to use red crosshair symbols representing clinics instead of circles.\nClean Up Resources After completing the demo, follow these steps:\nIn the Amazon S3 console, open the bucket created by the CloudFormation stack and delete the data-with-geocode.csv file. In the AWS CloudFormation console, delete the demo stack to remove created resources. Conclusion This article has guided you through setting up Redshift Serverless to use geospatial data in your data lake to enhance maps in ArcGIS Pro. This technique helps builders and GIS analysts leverage existing datasets in the data lake and transform them in Amazon Redshift to enrich data before presenting it on maps. We also showed how to protect your data lake using Lake Formation, discover geospatial datasets with AWS Glue, and visualize data in ArcGIS Pro.\nFor more best practices on storing geospatial data in Amazon S3 and querying with Amazon Redshift, see How to partition your geospatial data lake for analysis with Amazon Redshift. We welcome your feedback in the comments section.\nAbout the Authors Jeremy Spell is a cloud infrastructure architect working at AWS Professional Services. He enjoys designing and building solutions for customers. In his spare time, Jeremy enjoys making Texas-style BBQ and spending time with family and church community.\nJeff Demuth is a solutions architect who joined AWS in 2016. He focuses on the geospatial community and is passionate about Geographic Information Systems (GIS) and technology. Outside of work, Jeff enjoys traveling, building IoT applications, and tinkering with new gadgets.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.3-knowledge-base/5.3.1-create-kb/","title":"Initialize Knowledge Base","tags":[],"description":"","content":"Target We will use the Amazon Bedrock Wizard to set up the entire RAG architecture. This process will connect the S3 data source, the Embedding model, and automatically initialize the Vector storage (OpenSearch Serverless).\nImplementation Steps Log in to the AWS Management Console and access the Amazon Bedrock service. In the left-hand menu, select Knowledge bases. Click the Create knowledge base button in the top right corner of the screen. Step 1: Configure Knowledge Base\nOn the first configuration screen:\nKnowledge base name: Enter knowledge-base-demo Knowledge Base description - optional: Enter Knowledge Base from AWS Overview (This section requires you to describe the data you have previously uploaded to S3). IAM permissions: Select the option Create and use a new service role. Service role name: Keep the default value suggested by AWS (starting with AmazonBedrockExecutionRoleForKnowledgeBase_...). Click Next. Step 2: Configure Data Source\nConnect to the S3 Bucket containing the documents:\nData source name: Enterknowledge-base-demo S3 URI:\nClick the Browse S3 button. In the pop-up window, select the bucket rag-workshop-demo you created in the previous section. Click Choose. Keep Default configurations. Click Next.\nStep 3: Storage \u0026amp; Processing\nThis is the most critical step to define the AI model and vector storage location:\nEmbeddings model:\nClick Select model. Select model: Titan Embeddings G1 - Text v2. Vector Store:\nVector store creation method: Choose Quick create a new vector store - Recommended Vector store type - new: Choose Amazon OpenSearch Serverless Note: This option allows AWS to automatically create an Amazon OpenSearch Serverless cluster to store data, saving you from manual infrastructure management. Click Next. Step 4: Review and Create Knowledge Base\nReview all configuration information on the Review page. Ensure the S3 URI and Model items are correct. Scroll to the bottom of the page and click the Create knowledge base button. Step 5: Wait for Initialization\nAfter clicking Create, the system will begin the background infrastructure initialization process for the Vector Store.\nWait time: Approximately 2 - 5 minutes. Note: Please do not close the browser during this time. Success: When the screen displays a green notification \u0026ldquo;Knowledge base created successfully\u0026rdquo;, you have completed this step and are ready for the next section. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Huynh Bao Minh\nPhone Number: 078 222 4 999\nEmail: baominhbrthcs@gmail.com\nUniversity: FPT University Campus Ho Chi Minh\nMajor: Artificial Intelligence\nClass: SE193028\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Overview In this workshop, we will focus on building an intelligent AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; and answering questions based on proprietary enterprise data (RAG technique).\nThe main objective is to establish a fully automated and serverless data processing workflow, consisting of the following steps:\nIngestion: Loading source documents into the system. Indexing: Converting text into vectors and storing them for retrieval. Retrieval \u0026amp; Generation: Configuring the AI model to search for relevant information and generate answers to user questions. 💡 Highlight: This solution allows you to eliminate the need to manage any server infrastructure, optimizing costs and operational time.\nContents RAG Explanation Service Introduction "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.2-prerequiste/5.2.1-model-access/","title":"Verify Model Access","tags":[],"description":"","content":"Overview According to AWS\u0026rsquo;s new policy, Foundation Models are often automatically enabled. However, for third-party partner models like Anthropic (Claude), first-time users in a new Region must declare usage information (Use Case) to be able to invoke the model.\nEnsure your AWS account has permission to access and use the Anthropic Claude 3 Sonnet model. This is a mandatory step to avoid AccessDenied errors when the Chatbot operates later. If this is the first time you are using this model in a new Region, you need to declare the intended use (Use Case).\nAccess Check We will perform a quick test (Test Run) to ensure your account is ready.\nIn the search bar, access the Amazon Bedrock.\nStep 1. Access Chat Playground\nIn the left menu of the Bedrock Console, find Playgrounds. Click Chat. Step 2. Select Test Model\nClick Select model (above the chat box). Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet). Throughput: Select On-demand. Click Apply. Step 3. Send Activation Message\nIn the chat box: Enter Hello.\nClick Run.\nObserve result:\nIf AI answers: Success (Proceed immediately to section 5.2.2). If a red error or \u0026ldquo;Submit use case details\u0026rdquo; popup appears: Information declaration required (Continue to step 4 below).\nStep 4. Submit Use Case (Only perform if error occurred in step 3)\nClick Submit use case details (in the error message). Fill in the form: Company Name: Enter Personal Learning. Company website URL: Enter https://daihoc.fpt.edu.vn/ Industry: Select Education. Select Intended Use Describe\u0026hellip; Enter Research \u0026amp; Development. Click Submit. Wait 1 minute, return to the chat box, Click Run on the Hello message again to confirm success. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect with teammates and get acquainted with the Champions in the First Cloud Journey. Learn about the AWS services provided to customers. Complete the Lab as well as the knowledge in Module 1 of FJC 2025. Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon - Review AWS rules \u0026amp; regulations.\n- Team networking \u0026amp; grouping.\n- Plan study roadmap \u0026amp; internship project.\n- Create a progress tracking sheet. 08/09/2025 08/09/2025 Module 01 Tue - Understand Cloud Computing concepts \u0026amp; benefits.\n- AWS differentiators.\n- The Cloud adoption journey. 09/09/2025 09/09/2025 Module 01 Wed - Study AWS Global Infrastructure \u0026amp; Management Tools.\n- Cost optimization strategies.\n- Lab 01: Create Account, setup MFA, create Admin Group/User, test AWS Support. 10/09/2025 10/09/2025 Module 01 Thu - Lab 07: Create Budgets (Cost, Usage, RI, Savings Plan).\n- Resource termination.\n- Budget types \u0026amp; use cases. 11/09/2025 11/09/2025 Module 01 Fri - Lab 09: Explore AWS Support plans.\n- Access Support Center \u0026amp; open support cases. 12/09/2025 12/09/2025 Module 01 Week 1 Achievements: Understand what AWS is and the concepts and services AWS provides:\nCloud computing The differences of AWS How to start a cloud journey AWS global infrastructure AWS Services management tools How to optimize costs on AWS Successfully created and configured an AWS Free Tier account.\nGet familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nInstalled and configured AWS CLI on the computer including:\nAccess Key Secret Key Default Region MFA for the account Admin Group Admin User AWS Support Know how to set up Budgets:\nCost Budget Usage Budget RI Budget Saving Plan Budget Clean resources Clearly understand the support packages and how to access AWS Support:\nBasic Package Developer Package Business Package Enterprise Package "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.1-workshop-overview/5.1.1-whatisrag/","title":"What is Retrieval-Augmented Generation (RAG)","tags":[],"description":"","content":"Brief Definition RAG (short for Retrieval-Augmented Generation) is a technique or software architecture in the field of Artificial Intelligence (AI), designed to optimize the output of a Large Language Model (LLM).\nIn essence, RAG is a combination of two mechanisms:\nInformation Retrieval Mechanism: Searching for data from a highly reliable External Knowledge Base. Text Generation Mechanism: Using the LLM\u0026rsquo;s language understanding and synthesis capabilities to generate natural responses. The goal of RAG is to provide the LLM with accurate, up-to-date, and specific context, helping the model overcome the limitations of static training data.\nWhy is RAG needed? Traditional LLM models often face 3 major problems that RAG can solve:\nInformation Updates (Freshness): The LLM does not need Re-training or Fine-tuning yet can still answer the latest information, simply by updating the search database. Data Ownership (Proprietary Data): Allows AI to answer questions regarding private enterprise data (internal documents, code base, customer information) that the original model does not know. Authenticity (Grounding): Minimizes \u0026ldquo;Hallucination\u0026rdquo; (AI fabricating information) by forcing the AI to cite or rely on actual text passages found. Operational Architecture The process of handling a question in RAG proceeds as follows:\nStep Name Action Description 1 Retrieval (Truy xuất) The system searches for text segments most relevant to the question in the data repository (usually using a Vector Database). 2 Augmentation (Tăng cường) Combine the user\u0026rsquo;s question + The data just found into a complete \u0026ldquo;prompt\u0026rdquo;. 3 Generation (Tạo sinh) Send that prompt to the AI (LLM) for it to synthesize and write out the final answer for the user. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Get familiar with AWS/FCJ-2025 platforms\nWeek 2: Networking on AWS\nWeek 3: Compute VM Services on AWS\nWeek 4: Storage Services on AWS\nWeek 5: Security Services on AWS\nWeek 6: Database Services on AWS\nWeek 7: Learning knowledge on Skill Builder courses\nWeek 8: Learning about NLP and FastAPI\nWeek 9: Project deployment Sprint 01 - Research on Speech to Text and OCR models\nWeek 10: Project deployment Sprint 02 - Improvement and quality enhancement of models\nWeek 11: Project deployment Sprint 03 - Getting familiar with AWS support services and applying them to the project\nWeek 12: Project deployment Sprint 04 - Testing all models and error handling\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"AI-Driven Development Lifecycle: Reinventing Software Engineering Event Objectives Understand how AI can automate and optimize each stage of the Software Development Lifecycle (SDLC). Grasp the philosophy of AI augmenting humans rather than replacing them in the application development process. Directly observe how Amazon Q and other AI tools support developers from ideation, coding, to infrastructure deployment (Infrastructure as Code – IaC). Recognize the \u0026ldquo;AI-first development\u0026rdquo; trend – where AI becomes a natural part of future software development processes. Speaker List Mr.Toan Huynh - PMP, Senior Solutions Architect, AWS Ms.My Nguyen - Senior Solutions Architect, AWS Highlighted Content Challenges in AI-Assisted Programming The opening section presented limitations and challenges when integrating AI into programming:\nAI cannot yet handle projects with complex logic requiring deep understanding of business context. Developers struggle to control details in generated code if they don\u0026rsquo;t clearly describe objectives and scope. Code quality heavily depends on prompts and context provided by users. This is precisely why AI-DLC was created: to establish a structured process enabling more effective collaboration between AI and humans.\nAI in Development – How AI is Changing Software This section analyzed how AI is transforming the software industry:\nAI assists in code generation, technical documentation, API design, and automated testing. Developers transition from \u0026ldquo;code writers\u0026rdquo; to \u0026ldquo;AI orchestrators\u0026rdquo; — coordinators who guide and evaluate outputs. Tools like Amazon Q, GitHub Copilot, ChatGPT for Developers become central tools in modern dev team workflows. Introduction to AI-DLC AI-Driven Development Lifecycle (AI-DLC) is a software development approach with AI collaboration, where each step is designed to provide AI with specific context and objectives to generate more accurate results.\nInception\nBuild Context on Existing Codes – AI is \u0026ldquo;fed\u0026rdquo; with current source code to understand project structure. Elaborate Intent with User Stories – Developers describe requirements through user stories, clarifying objectives. Plan with Units of Work – Break down work into small units for AI to execute and generate code incrementally. Construction\nDomain Model (Component Model) – Build domain models or logical architecture diagrams. Generate Code \u0026amp; Test – AI generates code and automated tests based on planned information. Add Architectural Components – Add architectural components like APIs, data layers, logging, security. Deploy with IaC \u0026amp; Tests – Automatically deploy systems with Infrastructure as Code and integration tests. Each step provides additional \u0026ldquo;rich context\u0026rdquo; for the next step, helping AI understand the system more deeply and generate increasingly accurate results.\nCORE CONCEPTS – Three Core Principles Context Awareness – AI needs clear context about code, requirements, and domain to operate effectively. Collaborative Generation – Humans and AI collaborate: AI generates code, humans guide and review. Continuous Refinement – Iterative process to refine outputs and improve quality. Mob Elaboration Mob Elaboration is a method for expanding requirements (intent elaboration) through team collaboration:\nMultiple members jointly describe requirements, ask questions, and add information for AI. Helps AI understand more deeply about business, objectives, and complex project logic. This approach helps reduce misunderstanding risks, especially in large or multi-domain teams. 5-Stage Sequential Process of AI-DLC AI-DLC is executed through 5 stages:\nInception – Understand requirements, analyze systems. Construction – Create domain models and initial structure. Generation – Automatic code generation. Testing – Automate unit and integration testing. Deployment – Deploy applications with IaC and CI/CD pipelines. Each iteration helps AI learn more and improve output quality.\nDemo 1 – Hands-on Experience with AI DLC using Amazon Q The demo illustrated how to apply AI-DLC in practice through a small project:\nStarting from a simple idea → converting to user story describing business requirements. AI assists in dividing work (Units of Work) and detailed planning for each module. Participants can control AI through questions, checkboxes, and logical conditions, helping AI understand the scope of work. AI continues to generate code, write tests, create project structure, and automatically deploy trials. The demo clearly showed how AI and humans collaborate harmoniously: AI handles repetitive tasks, humans guide and make strategic decisions. Introduction to Kiro Kiro\u0026rsquo;s Philosophy\nThe next part of the workshop introduced Kiro, an intelligent development environment designed around the \u0026ldquo;AI-native development\u0026rdquo; philosophy – where AI is a core component, not just a support tool.\nKiro\u0026rsquo;s philosophy focuses on three main elements:\nDeep integration with development processes – AI not only assists in writing code but also participates in planning, managing context, and analyzing change impacts. Comprehensive project context understanding – Kiro maintains continuous state awareness of system structure, allowing AI to interact with the entire project rather than individual files. Intelligent control \u0026amp; collaboration – Developers can guide AI through contextual commands, ensuring each change has a clear purpose and is consistent with the system. Project Structure in Kiro\nUnlike traditional text editors like VSCode or JetBrains, Kiro is not just a code writing environment — it\u0026rsquo;s an AI workspace with structural awareness.\nProject structure in Kiro includes:\nContext Layer – Stores context, domain models, and relationships between modules. Task Layer – Manages Units of Work tracked and gradually completed by AI. AI Agent Layer – Each task (code, test, refactor, deploy) has a dedicated agent, creating a multi-agent – collaborative – parallel development model. Human-in-the-Loop Control – Developers can intervene at every step: confirm, modify, or reject AI outputs. This makes Kiro not just a code generation tool but a collaborative development ecosystem between humans and AI.\nDemo 2: Kiro – Applying AI-DLC In the demonstration, the speaker illustrated how Kiro operates AI-DLC seamlessly:\nUser inputs a basic business requirement, e.g., \u0026ldquo;build an event management system\u0026rdquo;. Kiro automatically analyzes intent, creates domain models, and breaks down into user stories. AI in Kiro generates corresponding modules, components, and test cases. Developers can interact through checkbox-based task control to confirm each part of the work. Finally, Kiro deploys the complete system with IaC and automated testing. The demo showed that AI-DLC is not just theory, but can be implemented practically within the Kiro environment — where AI, humans, and development processes merge into a unified system.\nEvent Experience Participating in the \u0026ldquo;AI DLC x Kiro: Reinventing Developer Experience with AI\u0026rdquo; workshop was an extremely valuable experience, helping me better understand how AI is deeply integrated into software development environments and how Kiro\u0026rsquo;s design philosophy brings a new approach to developers.\nLearning from Expert Speakers Speakers shared about AI DLC – a platform supporting AI-based software development, automating many SDLC processes. Additionally, the introduction to Kiro Editor provided deep insights into building a text editor in an AI-native direction rather than just \u0026ldquo;adding AI plugins\u0026rdquo; to old environments. I was particularly impressed with Kiro\u0026rsquo;s philosophy: minimalist, high-performance, focused on user experience and module-based scalability. Practical Technical Experience In learning:\nApply AI-DLC structure for personal projects Practice \u0026ldquo;Context Awareness\u0026rdquo; principle with AI assistants Build habit of writing clear requirements as user stories For future career:\nUnderstand modular, extensible, maintainable system design like Kiro Master Amazon Q and other AI tools effectively Recognize importance of providing quality context for AI Mindset shift:\nApproach problems with \u0026ldquo;AI-augmented\u0026rdquo; thinking Consider building custom tools with deep AI integration Always ask: \u0026ldquo;How can AI assist better at this step?\u0026rdquo; Applying Modern Tools Experiencing AI DLC on Kiro helped me better understand the capability of automating development processes, especially in steps like code generation, documentation, and debugging. I recognized the potential of building personal learning and working tools with intelligent suggestions, helping shorten development time and improve product quality. Kiro\u0026rsquo;s modular design concepts also suggested directions for designing flexible, scalable, and maintainable systems. Networking and Exchange The workshop created opportunities for me to connect with developers, AI researchers, and product designers, thereby learning more about the AI-augmented development trend. Through discussions, I learned much about how AI can play the role of creative collaborator, helping developers focus more on logic and system thinking rather than repetitive operations. Lessons Learned Participating in the \u0026ldquo;AI DLC x Kiro\u0026rdquo; workshop was a turning point in how I perceive AI\u0026rsquo;s role in software development.\nThe most important thing I learned wasn\u0026rsquo;t specific tools, but the necessary mindset shift:\nAI is not a tool for faster coding AI is a partner for better thinking and system design Structured processes (like AI-DLC) are more important than raw AI power The workshop also showed me the future of development tools - where AI-first architecture like Kiro will become standard, and developers need to prepare for this paradigm shift.\nInsights from AWS Solution Architects and hands-on experience with Kiro equipped me with a solid foundation to apply AI in my learning journey and future career in software engineering.\nSome Photos from the Event Group photo check-in after the event\nThis is the group check-in moment after the workshop ended. This event provided many valuable insights about how AI is reshaping development workflow.\nProfessional event space\nThe workshop was professionally organized with complete demo stations and networking opportunities. This was one of the important events that helped me deeply understand AI-driven development.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS VPC: Grasp the basic concepts of VPC (Virtual Private Cloud) as an isolated logical network environment, including key components like Subnets (Public and Private), Route Tables, and ENI. Traffic Control and Security: Learn to configure security layers (Security Groups and NACLs) and control network traffic flow to/from the Internet (Internet Gateway and NAT Gateway). Complex Network Connectivity: Differentiate and know how to use methods for connecting VPCs (VPC Peering) and the central connection model (Transit Gateway). Build a Hybrid Cloud Environment: Learn about solutions for connecting on-premises networks with AWS, including VPN (Site-to-Site) and private connections (AWS Direct Connect). Application Load Balancing: Understand the function of Elastic Load Balancing (ELB) and differentiate between various load balancer types (ALB, NLB, CLB, GLB) to ensure high availability and scalability for applications. Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon - Understand VPC architecture \u0026amp; core components: Subnets, Route Table, ENI, Endpoints.\n- Gateways: Internet Gateway, NAT Gateway.\n- Security: Security Group, NACL, Flow Logs. 15/09/2025 15/09/2025 Module 02 Tue - Network connectivity: VPC Peering, Transit Gateway.\n- Hybrid connectivity solutions: VPN (Site-to-Site, Client-to-Site) \u0026amp; AWS Direct Connect. 16/09/2025 16/09/2025 Module 02 Wed - Overview of Elastic Load Balancing (ELB).\n- Architecture \u0026amp; use cases for: Application (ALB), Network (NLB), Classic (CLB), and Gateway Load Balancer. 17/09/2025 17/09/2025 Module 02 Thu - Lab 03: Init VPC, configure Firewall \u0026amp; Site-to-Site VPN.\n- Lab 58: SSM Session Manager (EC2 connection, logs, Port Forwarding).\n- Lab 19: VPC Peering setup (Network ACL, Route tables, DNS). 18/09/2025 18/09/2025 Module 02 Fri - Lab 20: Transit Gateway configuration (multi-VPC connection, Route Tables).\n- Lab 10: Hybrid DNS (Outbound/Inbound Endpoints, Resolver Rules).\n- Extra study: AWS Advanced Networking Specialty. 19/09/2025 19/09/2025 Module 02 Research Link Week 2 Achievements: Explain what VPC is, its role in AWS, and its core components (Subnet, Route Table, ENI). Clearly differentiate between a Public Subnet (with an Internet Gateway) and a Private Subnet (using a NAT Gateway for Internet access). Compare and contrast the two main firewall mechanisms: Security Group (stateful, applies to ENI) and NACL (stateless, applies to Subnet). Present how to privately connect from a VPC to AWS services (like S3) without going over the Internet using a VPC Endpoint. Evaluate the pros and cons of two VPC connection solutions: VPC Peering (1:1 connection, no transitive support) and Transit Gateway (hub-and-spoke model, simplifies management). Describe methods for establishing a hybrid cloud connection, including Site-to-Site VPN (over the Internet) and AWS Direct Connect (private physical connection). Classify and select the appropriate Elastic Load Balancer type for specific scenarios: Application Load Balancer (ALB): For HTTP/HTTPS traffic (Layer 7), supports path-based routing. Network Load Balancer (NLB): For TCP/TLS traffic (Layer 4), requires ultra-high performance and static IP. Gateway Load Balancer (GLB): Used for integrating virtual appliances. Identify the necessary labs to reinforce knowledge of VPC, Peering, Transit Gateway, and related services. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.3-knowledge-base/5.3.2-sync-data/","title":"Check Vector Store and Data Synchronization","tags":[],"description":"","content":"Target Before the AI can answer, data must be ingested into the vector storage (Vector Store). We will perform a \u0026ldquo;Before and After\u0026rdquo; check to clearly see how Bedrock automatically encodes and stores data into OpenSearch.\nImplementation Steps Step 1: Check Vector Store (Empty State)\nWe will directly access Amazon OpenSearch Serverless to confirm that no data exists yet.\nIn the AWS Console search bar, type Amazon OpenSearch Service and select Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. Click on the Collection name newly created by Bedrock (usually named like bedrock-knowledge-data...). On the Collection details page, click the Open Dashboard button (located at the top right of the screen).\nNote: If asked to log in, use your current AWS credentials. In the OpenSearch Dashboard interface: Click the Menu (3 horizontal lines) icon in the top left corner. Select Dev Tools (usually located at the bottom of the menu list). In the Console pane (on the left), enter the following command to check data: GET _search { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} } } Click the Play (Run) button (small triangle next to the command line).\nResult: Observe the right pane, hits -\u0026gt; total -\u0026gt; value is 0.\nStep 2: Sync Data\nNow we will trigger Bedrock to read files from S3 and load them into OpenSearch.\nReturn to the Amazon Bedrock tab on the browser. Select Knowledge bases in the left menu and click on the KB name you just created. Scroll down to the Data source section, check the box (tick) next to the data source name (s3-datasource). Click the Sync button (Orange). Wait: This process will take 5 - 10 minutes depending on the sample document size. Wait until the Sync status column changes from Syncing to Available. Step 3: Re-check Vector Store (Populated)\nAfter Bedrock reports Sync completion, we return to the repository to verify the data has been successfully ingested.\nSwitch to the OpenSearch Dashboard tab (still open from Step 1). In Dev Tools, click the Play (Run) button again with the old command: GET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } Result: The hits -\u0026gt; total -\u0026gt; value section will be greater than 0 (e.g., 10, 20\u0026hellip; depending on the number of text chunks). You will see details of the vectors (number arrays) and text content stored in the _source field. Congratulations! You have completed building the \u0026ldquo;brain\u0026rdquo; for the AI. The data has been encoded and sits safely in the Vector Database, ready for retrieval.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.2-prerequiste/5.2.2-prepare-data/","title":"Prepare source data","tags":[],"description":"","content":"Overview Initialize an object storage (S3 Bucket) to hold original documents (PDF, Word, Text). This acts as the \u0026ldquo;Source of Truth\u0026rdquo; that the Knowledge Base will access to read, analyze, and synchronize knowledge for the AI. You can store knowledge related to your field to use in creating your own personal assistant or Chatbot.\nData Preparation We will create an S3 Bucket to store original documents, acting as the knowledge source for the Chatbot.\nStep 1. Create S3 Bucket\nAccess the S3 service from the search bar. AWS Region: Select United States (N. Virginia us-east-1). Click Create bucket. Configure Bucket information: Bucket Type: Click General purpose Bucket name: Enter rag-workshop-demo Object Ownership: Keep default ACLs disabled. Block Public Access settings: Keep default (Selected Block all public access). Scroll to the bottom of the page, Click Create bucket. Check that the S3 Bucket has been created successfully. Step 2. Upload sample documents\nThis is a sample document, relevant to overview of AWS cloud computing knowledge. You can use it to run demos or upload your data. PDF format file\nIn the Buckets list, Click on the bucket name you just created. Click Upload. In the Upload interface:\nClick Add files. Select the sample document file attached above or a file from your computer (PDF or Word file with a lot of text is recommended). Scroll to the bottom of the page, Click Upload.\nWhen you see the green \u0026ldquo;Upload succeeded\u0026rdquo; notification, Click Close. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"1. Objectives Before building the application, we need to establish a solid foundation. Similar to preparing ingredients before cooking, this section ensures your AWS account is ready with the necessary permissions and data.\nIn this section, we will complete 3 key initialization objectives:\nSelect Region: Set up the working environment in the United States N. Virginia (us-east-1) region to optimize connection speed and ensure service availability. Enable Model (Model Access): Check and ensure the account has permission to invoke the Anthropic Claude 3 model – the main linguistic \u0026ldquo;brain\u0026rdquo; of the system. Prepare Data (Data Setup): Initialize storage (S3 Bucket) and upload source documents to serve the knowledge ingestion process later. 2. Key Components In this preparation section, we will interact with the following components:\nAWS Management Console (Region Selector): The general management interface to switch the working Region to United States N. Virginia. Amazon Bedrock (Model Access \u0026amp; Playground): The place to manage access to Foundation Models and the chat tool to quickly test AI response capabilities. Amazon S3 (Simple Storage Service): Object storage service where we will create a Bucket to hold original document files (PDF, Word, Text). 3. Implementation Steps Check Model Access Prepare Source Data "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Personal Finance Management App (Vicobi) You can read the full proposal here: Vicobi Proposal 1. Executive Summary The Vicobi (Personal Finance Management App) project aims to provide an intelligent, modern, and highly automated personal financial management platform. Vicobi simplifies financial management through 4 main pillars:\nSmart Recording: Voice input and invoice scanning, eliminating manual data entry barriers. Goal-based Budgeting: Automated creation and flexible management of money jars. Analysis \u0026amp; Control: Provides visual reports and intelligent alert systems. AI Financial Assistant (Chatbot): Integrates AI Chatbot acting as an advisor, supporting inquiries and enhancing financial knowledge. From a technical perspective, Vicobi is built on a Microservices architecture using .NET Aspire and FastAPI, deployed on AWS Cloud, ensuring flexibility and data security. The development process follows the Agile/Scrum model (2 weeks/sprint during the main development phase), with the goal of completing MVP within 2 months of execution.\n2. Problem Statement Current Problem In today\u0026rsquo;s dynamic market, users face difficulties in controlling finances due to \u0026ldquo;behavioral inertia\u0026rdquo; — reluctance to manually record each transaction. Existing applications (like Money Lover, Misa Money Keeper) still rely heavily on manual input, causing \u0026ldquo;input fatigue\u0026rdquo; and high abandonment rates.\nSolution Vicobi solves the problem through high automation of the data entry process using AWS Cloud and Microservices:\nCore Technology: Integrates AI for Vietnamese voice processing (Voice-to-Text) and detailed invoice recognition (OCR). Optimized Architecture: Uses AWS ECS Fargate running Multi-container Task model (combining .NET Backend and AI Service) to reduce infrastructure costs while ensuring seamless communication. Modern Frontend: Uses Next.js hosted on Amazon S3 and distributed globally via Amazon CloudFront. Benefits and Return on Investment (ROI) The solution provides clear competitive advantages:\nUser Value: Reduces over 70% manual operations. Voice recognition accuracy reaches 90% and invoice extraction reaches 80%. Economic Efficiency: Maximizes AWS Free Tier usage (S3, CloudFront, Cognito). Lean operating budget around ~$60/month for infrastructure and ~$15/month for AI compute. ROI: Expected to achieve ROI within 6–12 months thanks to time savings and increased efficiency. Scalability: Microservices architecture ready for Mobile App integration or Open Banking. 3. Solution Architecture The system is designed with a distributed Microservices model, using API Gateway as the single entry point.\nTech Stack Details: Component Technology Details Frontend Next.js 16 App Router, TypeScript, Tailwind CSS, Zustand, React Query. Backend Core .NET Aspire Orchestrates Microservices (User, Wallet, Transaction, Report, Notification). AI Service FastAPI (Python) Handles Voice (PhoWhisper), OCR (Bedrock), Chatbot (RAG). Database Polyglot PostgreSQL, MongoDB, Elasticsearch, Qdrant (Vector DB). Messaging RabbitMQ Asynchronous communication between services. AWS Workflow: Access: Users access via Route 53, protected by AWS WAF and accelerated by CloudFront. Authentication: Amazon Cognito manages identity and issues JWT Tokens. API Processing: Requests go through API Gateway, connecting securely via AWS PrivateLink to Application Load Balancer (ALB). Compute: ALB distributes load to containers in ECS Fargate (located in Private Subnet). DevOps: CI/CD process fully automated by GitLab, builds images pushed to Amazon ECR and updates tasks on ECS. 4. Technical Deployment Implementation Phases The project spans 4 months (including internship):\nMonth 0 (Pre-internship): Ideation and overall planning. Month 1 (Foundation): Learn AWS, upgrade .NET/Next.js/AI skills. Set up VPC, IAM. Month 2 (Design): Design High-level \u0026amp; Detailed architecture on AWS. Month 3-4 (Realization): Coding, Integration Testing, Deploy to AWS Production, set up Monitoring. After Month 5: Research and develop Mobile App. Detailed Technical Requirements: Frontend: Deploy Next.js 16 on S3 + CloudFront. Use Origin Access Control (OAC) to secure bucket. Backend: Use .NET Aspire to manage Cloud-native configuration. Database-per-service: PostgreSQL \u0026amp; MongoDB. Elasticsearch for complex transaction search. Background Jobs: Use Hangfire. AI Service Pipelines: Voice: Preprocessing with Pydub, PhoWhisper-small Model (VinAI) for Vietnamese. OCR: Amazon Bedrock (Claude 3.5 Sonnet Multimodal) to accurately extract invoice information. Chatbot (RAG): Knowledge Base stored in Qdrant, generates responses via Amazon Bedrock (Claude 3.5 Sonnet). Security: Data encryption in transit (HTTPS/TLS 1.2+) and at rest (AES-256). Secrets management not deeply integrated (currently at MVP level), will upgrade to AWS Secrets Manager in the future. 5. Timeline \u0026amp; Milestones (Sprints) The main execution phase is divided into 4 Sprints:\nSprint 1: Core Foundation Authentication (Cognito), Wallet Management, Spending Jars. Sprint 2: Core Features Transactions (CRUD), AI Voice Processing. Sprint 3: Analytics Reports/Charts, Notification System (SES), Message Broker. Sprint 4: Stabilization Integration Testing, UI Refinement, Deploy to AWS ECS \u0026amp; CloudFront. Testing \u0026amp; Go-live: Domain Configuration, SSL, Monitoring Dashboard, UAT and project defense. 6. Budget Estimation Based on detailed cost estimates for the MVP phase.\nYou can review the detailed cost estimation by downloading the following files: 📊 CSV Pricing File 💾 JSON Pricing File\nAWS Service Component / Usage Cost (USD/month) Elastic Load Balancing Application Load Balancer $18.98 Amazon ECS Fargate (vCPU \u0026amp; Memory) $17.30 Amazon VPC VPC Endpoints \u0026amp; NAT $10.49 AWS WAF Web ACL \u0026amp; Requests $7.20 Amazon API Gateway API Calls \u0026amp; Data Transfer $2.50 Amazon CloudFront Data Transfer Out $2.00 Amazon ECR Storage $1.00 Amazon Route 53 Hosted Zones $0.54 Amazon S3 Standard Storage $0.34 TOTAL AWS COST ~$60.35 Other Costs:\nCategory Details Cost (USD/month) AI Compute / Tooling Gemini API, Amazon Bedrock ~$15.00 PROJECT TOTAL ~$75.35 / month (Based on On-Demand pricing in Singapore region - ap-southeast-1)\n7. Risk Assessment Main Risks: User information leakage (Impact: High), AWS Region connection loss (Impact: High), AI misrecognition (Impact: Medium). Mitigation Strategies: Security: AES-256 encryption, HTTPS, IAM Least Privilege, AWS WAF. High Availability: Multi-AZ deployment for ECS and ALB. AI: Continuously improve model with real data. Resilience: Use internal RabbitMQ for asynchronous processing and retry. Disaster Recovery Plan: Use IaC (Infrastructure as Code) for rapid infrastructure restoration. 8. Expected Results \u0026amp; Team Expected Results of the Project Automated financial data entry: The application helps users avoid manual entry, just take a photo of the invoice or record a voice for the system to automatically classify spending. Intuitive financial management: Users can view spending charts, monthly reports, and receive savings suggestions based on consumer behavior. Minimal user experience: Friendly web interface, modern design, optimized for mobile devices and suitable for people new to financial management. Stable, scalable system: Microservices architecture makes it easy to add new features such as spending reminders, AI predictive analysis, or expand to a mobile app. Improving development team skills: Project members have practical access to DevOps processes, CI/CD implementation, and cloud-based application optimization. Project Limitations Vietnamese AI model is still limited: The ability to recognize regional voices or handwritten invoices has not yet achieved high accuracy. No separate mobile application: The MVP version only supports the web platform, there is no native mobile app. Implementation Team: Name Role Email Le Vu Phuong Hoa Backend Developer (Leader) hoalvpse181951@fpt.edu.vn Nguyen Van Anh Duy AI Developer (Member) duynvase181823@fpt.edu.vn Uong Tuan Vu Frontend Developer (Member) vuutse180241@fpt.edu.vn Tran Huynh Bao Minh AI Developer (Member) baominhbrthcs@gmail.com Mentor Support:\nNguyen Gia Hung - Head of Solution Architects Van Hoang Kha - Cloud Security Engineer "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.1-workshop-overview/5.1.2-services/","title":"Services","tags":[],"description":"","content":"The solution architecture is built upon the coordination of the following 4 key service components:\nKnowledge Bases for Amazon Bedrock This is a fully managed capability that helps connect Foundation Models to the enterprise\u0026rsquo;s internal data sources.\nRAG workflow automation: Manages the entire end-to-end workflow, including ingestion, chunking, embedding, and retrieval. Contextual connection: Enables AI applications to answer questions based on private data rather than relying solely on generic training data. No infrastructure management: Eliminates the need to build and maintain complex data pipelines. Amazon Simple Storage Service (Amazon S3) An object storage service with scalability, 99.999999999% (11 nines) data durability, and top-tier security.\nData Source Role: Acts as the \u0026ldquo;source of truth\u0026rdquo;. Document storage: Contains unstructured files such as PDF, Word, or Text that the business wants the AI to learn. Synchronization: The Knowledge Base will periodically scan this S3 bucket to synchronize and update the latest knowledge. Amazon OpenSearch Serverless A serverless deployment option for Amazon OpenSearch Service that helps run search and analytics workloads without managing clusters.\nVector Store Role: Stores vector embeddings generated from original documents. Semantic Search: Performs similarity search algorithms (k-NN) to identify text segments with meanings closest to the user\u0026rsquo;s question. Auto-scaling: Automatically adjusts compute and storage resources based on actual demand. Amazon Bedrock Foundation Models (FMs) Provides access to leading AI models via a unified API. In this architecture, we use two types of models with distinct roles:\nEmbedding Model (Amazon Titan Embeddings v2): Converts text (documents from S3 and user questions) into numerical vectors. Enables computers to compare semantic similarity between text segments. Text Generation Model (Anthropic Claude 3): Acts as the reasoning \u0026ldquo;brain\u0026rdquo;. Receives the question along with contextual information retrieved from the Vector Store. Synthesizes information and generates natural, accurate answers with source citations. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"WORKSHOP \u0026ldquo;DATA SCIENCE ON AWS PLATFORM\u0026rdquo; Event Objectives Introduce an overview of the ecosystem of artificial intelligence (AI) services available on AWS. Guide practical processes for building and training AI models using Amazon SageMaker. Demonstrate how to deploy AI models from laboratory to production and integrate them into applications through APIs. Speaker List Mr. Van Hoang Kha - Cloud Solutions Architect, Leader of AWS User Group (Sharing solution architecture perspective). Mr. Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder (Sharing operations and deployment perspective). In-Depth Content The Importance of Cloud Computing in Data Science Analysis of the core role of Cloud Computing: Not just storage, Cloud provides unlimited computing power to process Big Data and train complex AI models that personal computers struggle to handle.\nEfficiency comparison between Cloud and On-premise (Physical servers):\nCloud: Strengths lie in Elasticity — can scale resources up/down instantly, extremely fast deployment, convert CAPEX (Capital Expenditure) to OPEX (Operating Expenditure), and easily integrate new technologies. On-premise: Faces major barriers in initial hardware costs, difficulty expanding infrastructure when data surges, and expensive maintenance personnel. AWS plays the role of backbone for the entire Data Science pipeline: Provides a closed-loop process from raw data collection, cleaning, training to when the model is put into practical use.\nAI Architecture Layers on AWS AWS categorizes AI services into 3 separate layers, designed to suit different user audiences with varying technical levels and control needs:\n1. AI Services (Fully Managed Service Layer)\n\u0026ldquo;Instant noodle\u0026rdquo; solution for Developers who want to integrate intelligence into applications without deep ML algorithm knowledge.\nThese are pre-trained models by AWS with billions of data points.\nUsers only need to send data via API and receive analysis results.\nNotable services:\nAmazon Comprehend: Understand and analyze text meaning, user sentiment (NLP). Amazon Translate: Remove language barriers with automatic translation capabilities. Amazon Textract: \u0026ldquo;Read\u0026rdquo; documents, extract text and tables from scanned files/images. Amazon Rekognition: Computer vision, recognize objects, faces in images/videos. Amazon Polly: Natural artificial voice reading (Text-to-Speech). Amazon Bedrock: Gateway to today\u0026rsquo;s leading Large Language Models (LLMs). Benefits: Extremely fast time-to-market, no effort building models from scratch.\n2. ML Services (Semi-Managed Layer)\nPowerful tool for Data Scientists \u0026amp; ML Engineers needing professional environments to build their own models.\nAmazon SageMaker is the heart of this layer, providing a unified platform to manage ML model lifecycle (ML Ops).\nKey modules:\nData Wrangler: Minimize time for data preparation and cleaning (typically 80% of project time). Feature Store: Repository for storing data features for reuse, avoiding waste of computing resources. AutoML (SageMaker Autopilot): Automatically test multiple algorithms to find the best model without manual intervention. Model Registry \u0026amp; Monitoring: Manage model versions and monitor model accuracy in real-time (avoid model drift phenomenon). Benefits: Perfect balance between convenience and deep customization capability in training processes.\n3. AI Infrastructure (Self-Managed Infrastructure Layer)\nFor researchers needing deep hardware intervention and lowest-level optimization.\nProvides \u0026ldquo;building blocks\u0026rdquo; to build custom AI systems:\nAmazon EC2 P5/G6/Inferentia: Virtual machines with dedicated chips (GPU/ASIC) for extremely high computing performance. Amazon EKS / ECS: Container management for large-scale ML applications. AWS Lambda: Run inference code serverless, cost-effective for small tasks. Amazon S3 / EFS: Massive \u0026ldquo;Data Lake\u0026rdquo; storage system. Benefits: Not limited by templates, thoroughly optimize costs for super-large systems, but requires high DevOps skills.\nPopular AI Services Supporting Students \u0026amp; Researchers 1. Amazon SageMaker\nAn IDE (Integrated Development Environment) dedicated to ML on cloud:\nIntegrates everything from raw data processing to hyperparameter tuning. Provides CI/CD capability for Machine Learning, automating train and deploy processes. Supports Notebooks (Jupyter) familiar to students. 2. Amazon Comprehend\nBrings \u0026ldquo;reading comprehension\u0026rdquo; capability to applications:\nSentiment analysis: Know if customers are happy, angry, or satisfied through emails/comments. Entity recognition: Automatically detect names of people, places, organizations in text. Security: Automatically find and mask personal information (PII) in data. 3. Amazon Translate\nHigh-quality translation based on Deep Learning (Neural Network). Ability to customize specialized vocabulary (e.g., correctly translate medical or technical terms). Helps expand application user base globally. 4. Amazon Textract\nFar surpasses traditional OCR technology thanks to document structure understanding. Preserves table and form formatting when extracting, helps digitize administrative documents quickly and accurately. Standard Data Science Process on AWS Ingest \u0026amp; Store: Collect data from multiple sources into Amazon S3 \u0026ldquo;warehouse\u0026rdquo;. Prepare: Clean and standardize data using AWS Glue or Lambda. Train \u0026amp; Tune: Use SageMaker to train and optimize algorithms. Deploy: Package model into API Endpoint or integrate into application. Monitor: Use CloudWatch to monitor system health and model prediction quality. Demo 1: Optimizing Workflow with Low-Code/No-Code Objective: Prove that technical barriers in AI are gradually being removed thanks to visual tools.\nTool: Amazon SageMaker Canvas (Drag-and-drop interface).\nImplementation process:\nUpload raw dataset to S3. Use graphical interface to define processing flow: Input -\u0026gt; Handle missing data -\u0026gt; Select target column -\u0026gt; Train. System automatically runs experiments and returns evaluation metrics (Accuracy, F1-score\u0026hellip;) in easy-to-understand charts. Significance: Helps students and Business Analysts create value from data immediately without writing thousands of lines of Python code.\nDemo 2: From Model to Real Application (Deployment) Objective: Illustrate the \u0026ldquo;bridge\u0026rdquo; connecting mathematical models and end users.\nTools: SageMaker Endpoint combined with API Gateway and Lambda.\nImplementation process:\nConvert trained model into an HTTP Endpoint (network address). Set up API Gateway to receive external requests (e.g., from mobile app). Process intermediate logic using AWS Lambda (serverless) to call model and return results to users. Significance: Shows the full picture of making AI products: Model is just one part, integration and serving the model creates complete products.\nComparison Table: Cloud vs. On-premise (Performance \u0026amp; Economics Perspective) Criteria Cloud (AWS) On-premise (Private servers) Scalability Unlimited \u0026amp; Instant: Auto-scale according to actual traffic. Rigid: Limited by current hardware quantity. Cost Model OPEX (Operating Expenditure): Pay as you go, no waste. CAPEX (Capital Expenditure): Must invest large sum buying equipment, depreciation risk. Deployment Speed Minutes: Just a few clicks to have servers. Weeks/Months: Must order, install, setup OS. System Maintenance AWS handles: Focus entirely on application development. DIY: Spend IT personnel on electricity, cooling, hardware operations. Student-friendly High: Utilize Free Tier for free learning. Low: Requires expensive high-spec machines. General Conclusion AWS provides a comprehensive and seamless ecosystem, blurring the gap between academic learning and practical applications. Whether beginners or large enterprises, AWS has appropriate tool sets (Layers) to solve data problems. Personal Reflections After the Event The \u0026ldquo;AI Services on AWS for Data Science\u0026rdquo; workshop not only provided theoretical knowledge but also opened up new thinking about technology approaches, helping me better define the path from data research to product development.\nExpanded thinking thanks to experts Clearly understand why the world is shifting to Cloud: It\u0026rsquo;s liberation from infrastructure burden to focus on core value which is data. Grasped the overall picture of 3 AI layers, thereby knowing how to choose appropriate services for each stage of personal projects. Visualizing knowledge through Demos Demo 1 (Canvas): Impressed with AI \u0026ldquo;democratization\u0026rdquo; capability. Training models now is as visual as assembling lego, helping validate ideas extremely fast. Demo 2 (Deployment): This is the piece I often miss. Understanding how to create an API for others to use my model is a turning point from \u0026ldquo;doing homework\u0026rdquo; to \u0026ldquo;making products\u0026rdquo;. Accessing advanced technology Services like Comprehend or Textract show the power of Pre-trained models. They help solve difficult problems (like reading invoices, sentiment analysis) in an instant without building complex models yourself. Connection value Opportunity to discuss Cost optimization issues is very practical, helping students like me know how to use Cloud without \u0026ldquo;burning money\u0026rdquo;. Networking with industry professionals gives me more motivation and clearer career direction. Core lessons Cloud First: Thinking of putting everything on cloud is an inevitable trend in modern Data Science. Practicality: An AI model only has value when deployed and solving problems for end users. AWS Ecosystem: A massive tool repository that I need to continue exploring to enhance my capabilities. Some images captured at the event Image of knowledge transfer from 2 AWS representatives\nImage of FPTers team check-in after the event\nThis is the check-in moment of all interns at AWS after the workshop ended.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand comprehensive knowledge of virtual server (Compute VM) services on AWS. Focus on the core service Amazon EC2, including configuration selection (Instance Types), storage types (EBS, Instance Store), and automation (User data, Auto Scaling). Learn about related services such as Amazon Lightsail (low-cost service), shared file storage solutions (EFS for Linux and FSx for Windows/Linux), and the AWS MGN application migration service to migrate servers to AWS or build Disaster Recovery. Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Amazon EC2:\n- Architecture, Instance Types, AMI, Key Pairs.\n- Storage: EBS vs. Instance Store.\n- Configuration: User Data, Meta Data.\n- Management: Auto Scaling \u0026amp; Pricing Options. 22/09/2025 22/09/2025 Module 03 Tue Amazon Lightsail: Low-cost VPS \u0026amp; VPC Peering.\nAmazon EFS/FSx:\n- EFS: Network file storage for Linux (NFS).\n- FSx: Storage for Windows/Linux (SMB) \u0026amp; deduplication.\nAWS MGN: Server Migration \u0026amp; Disaster Recovery (DR) setup. 23/09/2025 23/09/2025 Module 03 Wed Lab 000004: Basic EC2 operations (Create instance, Snapshot, App install).\nLab 000027: Resource Management via Tags and Resource Groups. 24/09/2025 24/09/2025 Module 03 Thu Lab 000008: Resource Monitoring with CloudWatch (Agent, Dashboard).\nLab 000006: Deploy Auto Scaling Group (Launch Template, Target Group, Load Balancer). 25/09/2025 25/09/2025 Module 03 Fri Lab 000045: Amazon Lightsail Deep Dive (LB, RDS, Migrate to EC2).\nExtra Study: Microsoft Workloads on AWS, Linux/Windows OS administration. 26/09/2025 26/09/2025 Module 03 Research Link Week 3 Achievements: EC2 Service: Clearly understand that EC2 is the core virtual server service of AWS. EC2 Configuration Techniques: Know how to select Instance Type (CPU, RAM, Network configuration) and use AMI to provision the operating system for the server. EC2 Security Techniques: Understand how to use Key Pair (public/private key) to encrypt login information, instead of using passwords. Storage Service (Storage): Clearly differentiate the 2 main disk storage types for EC2: EBS (Elastic Block Store): Is a network drive, operates independently, data is replicated 3x within 1 AZ (99.999% availability), can be backed up using snapshots. Instance Store: Is a physical drive (NVME) with extremely high speed, but the data is temporary (will be erased when EC2 is stopped), often used for cache/buffer or swap. Automation Techniques (Automation): Know how to use User Data to run a script once when the server boots up (e.g., install a web server). Understand what Meta Data is and how to use it to retrieve information (IP, hostname) about the server from within itself, used for automation scripts. Scaling Techniques (Scaling): Master the concept of EC2 Auto Scaling to automatically increase (scale-out) or decrease (scale-in) the number of servers based on load (e.g., when ActiveConnectionCount is high or low). Cost Optimization Techniques (Pricing): Recognize the 4 EC2 pricing models: On-demand (per hour/second, most expensive), Reserved Instance (1-3 year commitment), Saving Plans (1-3 year commitment, more flexible), and Spot Instance (low price, utilizes surplus resources but can be reclaimed). Lightsail Service: Understand Amazon Lightsail is a low-cost, simplified VM service, suitable for light workloads, and know how to peer it with a VPC. File Storage Service (File Storage): Differentiate between the 2 shared file storage services for multiple servers: EFS (Elastic File System): Used for Linux (NFSv4 protocol), charges based on storage used. FSx: Used for Windows/Linux (SMB protocol), supports deduplication feature to reduce costs. Migration Service (Migration): Understand AWS MGN is a service to migrate servers from on-premise to AWS or to build a low-cost Disaster Recovery (DR) system via a staging area. Hands-on: Understand the basic hands-on steps with EC2 (create, snapshot), deploy a complete Auto Scaling Group (with Load Balancer), and get familiar with Lightsail. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.3-knowledge-base/","title":"Create and Configure Knowledge Base","tags":[],"description":"","content":"Objectives After completing the environment and data preparation, the next step is to set up the core component of the RAG architecture. In this section, we will initialize the Knowledge Base, acting as an intelligent intermediary mechanism that connects unstructured data sources with the reasoning capabilities of foundation models.\nWe will accomplish 3 key technical objectives:\nEstablish an Automated Pipeline: Configure the Knowledge Base to automate the entire RAG data processing workflow (including extraction, text chunking, and vector creation) to eliminate manual processing tasks. Initialize Vector Store: Deploy a collection on Amazon OpenSearch Serverless to store semantic vectors, serving accurate and efficient information retrieval. Data Synchronization (Data Ingestion): Perform the initial data ingestion process, converting static documents from S3 into searchable vectors within the system. Key Components During this configuration process, we will interact with and connect the following services:\nKnowledge Bases for Amazon Bedrock: A managed service acting as the orchestrator of data flow, connecting information sources, and executing semantic queries. Amazon Titan Embeddings G1 - Text v2: A specialized model for converting text data into numerical vectors (Embeddings) with high accuracy and multi-language support. Amazon OpenSearch Serverless: A fully managed vector database responsible for storage and executing similarity search algorithms (k-NN). Implementation Steps Initialize Knowledge Base Check Vector Store and Sync Data "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Some blogs that I have been translated so far:\nBlog 1 - New capabilities in Amazon SageMaker AI continue to transform how organizations develop AI models This blog introduces the latest features in Amazon SageMaker AI aimed at accelerating the development and deployment process for large-scale generative AI models. You will learn how Amazon SageMaker HyperPod optimizes performance through observability features that help reduce troubleshooting time from days to minutes, as well as the feature for quickly deploying open-weight models from JumpStart for inference. The article also instructs on how to seamlessly connect local development environments (like Visual Studio Code) with powerful cloud infrastructure, and how to use fully managed MLflow 3.0 to more effectively track experiments and evaluate the performance of generative AI applications.\nBlog 2 - Improving weather forecasting accuracy using Amazon AppStream 2.0 graphic-intensive instances This blog introduces a solution to enhance weather forecasting capabilities by deploying the Common AWIPS Visualization Environment (CAVE) application on Amazon AppStream 2.0. You will learn how this service allows meteorologists to access specialized graphic tools from any device, which helps eliminate the reliance on expensive hardware and optimizes costs through a pay-per-use GPU consumption model. The article also provides technical steps ranging from creating a Linux image and optimizing the application manifest to configuring the fleet and user authentication, ensuring high performance and data security for critical forecasting tasks.\nBlog 3 - Geospatial data lakes with Amazon Redshift This blog introduces a solution for building a secure geospatial data lake by leveraging the capabilities of Amazon Redshift and AWS Lake Formation. You will learn how to store large-scale map data on Amazon S3 and perform direct queries through Redshift Serverless without data movement, helping to optimize storage costs and analysis performance. The article provides detailed technical guidance on the process, from infrastructure deployment and using AWS Glue for metadata management, to applying security tags in Lake Formation to control column-level access, ensuring that sensitive information (such as PHI medical data) is absolutely protected. Finally, you will be guided on how to create geospatial views in Redshift and connect the ArcGIS Pro software to visualize the data on a map for real-world analytical needs.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI Event Purpose Master Prompt Engineering techniques to optimize input, helping models understand and execute user intent accurately. Leverage the power of Pretrained AI Services on AWS to quickly integrate intelligent features without building models from scratch. Deep understanding of RAG (Retrieval-Augmented Generation) architecture to solve hallucination problems and update private data for AI. Grasp the next technology wave: Agentic AI and how to use Amazon Bedrock AgentCore to transform AI Agents from proof of concept (POC) to production environment. Access Pipecat Framework to build virtual assistants with low-latency voice communication (Real-time). Speaker List Mr. Lam Tuan Kiet - Sr DevOps Engineer (FPT Software) - Expert in system operations and deployment. Mr. Danh Hoang Hieu Nghi - AI Engineer (Renova Cloud) - Expert in artificial intelligence solutions. Mr. Dinh Le Hoang Anh - Cloud Engineer Trainee (First Cloud AI Journey) - Sharing perspectives from someone starting their Cloud AI journey. Highlighted Content 1. Prompt Engineering \u0026amp; Foundation Models (Core Foundation) Before diving into complex systems, the event emphasized that \u0026ldquo;input quality determines output quality\u0026rdquo;. Effective communication with Foundation Models on Amazon Bedrock is the first step:\nZero-shot / Few-shot Prompting: Techniques to guide models by giving direct commands or providing a few sample examples (context) for AI to learn from that pattern and return desired results. Chain of Thought (CoT): An advanced technique helping AI handle complex reasoning tasks by requiring it to \u0026ldquo;explain step by step\u0026rdquo;. This reduces logical errors in answers. 2. Pretrained AI Services (AWS AI Services) This is the application layer helping developers not specialized in Machine Learning to still integrate AI into products through APIs:\nComputer Vision: Amazon Rekognition helps analyze images/videos, recognize objects and moderate content. Natural Language Processing: The trio of Amazon Translate (translation), Comprehend (sentiment/semantic analysis), and Textract (intelligent OCR - text extraction from documents). Audio Processing: Amazon Polly (convert text to natural speech) and Transcribe (convert speech to text). 3. RAG - Retrieval Augmented Generation RAG is a bridging solution helping AI \u0026ldquo;learn\u0026rdquo; enterprise-specific data without retraining (fine-tuning), solving the problem of outdated models or \u0026ldquo;hallucination\u0026rdquo;:\nEmbeddings (Vectorization): Use models like Amazon Titan Text Embeddings V2 to convert text into mathematical vectors, helping systems understand and search based on semantics (semantic search) rather than just keyword matching. Knowledge Bases for Amazon Bedrock: End-to-end managed service, automating complex steps: Document chunking -\u0026gt; Store in Vector Store -\u0026gt; Retrieve relevant data (Retrieval) -\u0026gt; Synthesize answers (Generation). 4. Evolution to Agentic AI (The Era of Task-oriented AI) GenAI is shifting from just \u0026ldquo;answering questions\u0026rdquo; to \u0026ldquo;taking action\u0026rdquo;. The event clearly categorized the development roadmap:\nGenAI Assistants: Virtual assistants performing single, repetitive tasks based on predefined rules. GenAI Agents: Goal-oriented AI with reasoning ability to choose appropriate tools to complete a sequence of work. Agentic AI Systems: Multi-agent ecosystem, highly autonomous, capable of coordinating with each other under minimal human supervision. The Challenge of \u0026ldquo;The Prototype to Production Chasm\u0026rdquo;: Why do many great Agent demos fail when deployed to production?\nPerformance \u0026amp; Scalability: Agents work slowly when processing long reasoning chains. Safety \u0026amp; Governance: Risks when Agents autonomously perform wrong actions (e.g., accidentally deleting database) or accessing sensitive data. Complexity: Difficulty maintaining context memory (Memory) across long working sessions. 5. Amazon Bedrock AgentCore: Solution for Bringing Agents to Market AgentCore was introduced as a complete infrastructure platform to solve the above problems, helping enterprises confidently deploy Agents:\nCore Components: Runtime \u0026amp; Memory: Provides stable execution environment and long-term \u0026ldquo;memory\u0026rdquo; capability of past interactions. Identity \u0026amp; Gateway: Strict identity management, ensuring Agents only execute within granted permissions. Code Interpreter: A safe \u0026ldquo;sandbox\u0026rdquo; allowing Agents to autonomously write and run Python code to process numerical calculations or draw accurate charts (instead of guessing numbers). Observability: Monitoring tools helping humans track each reasoning step (Trace) of Agents for debugging and optimization. Benefits: Helps Developers focus on business logic, reducing the burden of building supporting infrastructure. 6. Pipecat: Framework for Real-time AI Voice Introduction to an open-source Framework helping build natural human-machine communication applications (Multimodal):\nFeatures: Optimizes extremely low latency, a vital factor in voice communication. Pipeline Mechanism: WebRTC Input: Receives audio stream directly from browser/app. STT (Speech-to-Text): Translates speech to text instantly. LLM Processing: AI thinks and generates text responses. TTS (Text-to-Speech): Converts responses to speech. Output: Plays back to users. The key point is these steps occur almost in parallel (streaming) to create a seamless conversation experience. Detailed Experience in the Event The workshop helped me systematize knowledge and see a bigger picture of AI\u0026rsquo;s future.\n1. The Shift from \u0026ldquo;Question-Answer\u0026rdquo; to \u0026ldquo;Action\u0026rdquo; (Agentic AI) What impressed me most was the thinking about Agentic AI. Previously, I only viewed AI as an intelligent lookup tool. But with AgentCore, AI becomes a \u0026ldquo;digital employee\u0026rdquo; capable of autonomously planning and using tools (API, Code, Search) to solve problems completely. This is a quantum leap in usage value.\n2. Solving the \u0026ldquo;Production\u0026rdquo; Problem The sharing about \u0026ldquo;The Prototype to Production Chasm\u0026rdquo; was very practical. It explains why many AI projects die young. AWS providing security layers (Identity) and monitoring (Observability) in Bedrock Agent is the key for enterprises to dare trust giving AI authority to impact real systems.\n3. Potential of Voice AI with Pipecat The Pipecat demo showed the future of contactless communication. Combining WebRTC and LLM to create real-time conversations opens countless applications: from automatic customer service hotlines, virtual IELTS practice, to interview assistants.\nConclusion The \u0026ldquo;Generative AI \u0026amp; Agentic AI on AWS\u0026rdquo; workshop clearly outlined the AI capability development roadmap:\nPresent: We use RAG and Prompt Engineering to effectively exploit data. Near Future: We shift to Agentic AI, where autonomous systems (Autonomous Agents) will replace humans in operating complex processes. Tools: With AWS\u0026rsquo;s comprehensive ecosystem (Bedrock, AgentCore) and Open Source community (Pipecat, LangChain), technical barriers have been significantly reduced, making way for creativity in business solutions. Some images from participating in the event Image of over 400 participants attending the AWS Cloud Mastery Series #1 Event\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn comprehensive knowledge about the diverse storage services on AWS. Focus deeply on the core service Amazon S3 (Simple Storage Service), an object storage service, including its characteristics (like 11 nines durability, replication across 3 AZs), and Storage Classes. Learn about important features like Lifecycle Management, Versioning, and Static Website Hosting. Large-scale data migration solutions (the Snow Family), hybrid storage solutions connecting on-premise with the cloud (Storage Gateway), the centralized backup management service (AWS Backup), and the basic concepts and strategies for Disaster Recovery (DR). Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Amazon S3:\n- Architecture (Object storage, 3 AZ, durability).\n- Storage Classes \u0026amp; Lifecycle Management.\n- Features: Static Website, CORS, Versioning.\n- Security: Bucket Policies, ACLs, S3 Endpoints.\n- Performance optimization \u0026amp; S3 Glacier. 29/09/2025 29/09/2025 Module 04 Tue Snow Family: Bulk data migration (Snowball, Snowmobile) \u0026amp; Edge computing.\nAWS Storage Gateway: Hybrid cloud storage solutions (File, Volume, Tape Gateway) connecting On-premise to AWS. 30/09/2025 30/09/2025 Module 04 Wed Disaster Recovery (DR): RTO/RPO metrics and 4 AWS DR strategies.\nAWS Backup: Centralized backup management, scheduling, and retention policies for AWS services. 01/10/2025 01/10/2025 Module 04 Thu Lab 000057: S3 Basics (Create Bucket, Upload, Host Static Website).\nLab 000013: Configure AWS Backup Plan \u0026amp; Notification.\nLab 000014: VM Import/Export operations. 02/10/2025 02/10/2025 Module 04 Fri Lab 000024: Storage Gateway setup \u0026amp; File Sharing.\nLab 000025: Deploy Amazon FSx with AWS Managed AD.\nExtra Study: AWS Skill Builder (Block \u0026amp; Object Storage Learning Plans). 03/10/2025 03/10/2025 Module 04 Week 4 Achievements: S3 Service (Basics): Clearly understand that Amazon S3 is an object storage service, not block storage, operating on a WORM (Write Once, Read Many) model. Lesson on Durability: Know that S3 is designed for 11 nines (99.999999999%) of durability by automatically replicating data across 3 Availability Zones (AZs). S3 Cost Optimization Techniques: Differentiate between Storage Classes such as S3 Standard (frequent access), S3 Standard-IA (infrequent access), and S3 Glacier (long-term, low-cost archival, requires retrieval). S3 Automation Techniques: Know how to use Object Life Cycle Management to automatically transition data to cheaper tiers (e.g., from Standard to Glacier) over time. Understand Trigger Events (e.g., triggering a serverless function upon file upload). S3 Security Techniques: Differentiate between two access control mechanisms: S3 ACL (legacy mechanism) and S3 Bucket Policy (easier to define access permissions). Lesson on Data Protection (S3): Clearly understand the Versioning feature, which allows restoring previous versions of a file, helping to protect against accidental deletion or ransomware attacks. S3 Networking Techniques: Know how to use an S3 Endpoint to access S3 from within a VPC over the AWS private network without needing the Internet. Know how to host a Static Website on S3 and configure CORS. Data Migration Service (Migration): Recognize the Snow Family (Snowball, Snowmobile) as the physical solution for large-scale (Petabyte, Exabyte) data migration from on-premise. Hybrid Storage Service: Understand Storage Gateway as a hybrid storage solution, allowing on-premise applications to use protocols (NFS, SMB, iSCSI) to store data on S3/Glacier. Lesson on Disaster Recovery (DR): Understand the 2 basic concepts for designing DR: RTO (recovery time) and RPO (acceptable data loss). Backup Service (Backup): Know that AWS Backup is a centralized management service that helps automate backups (schedule, retention) for multiple AWS resources (EBS, RDS, EFS\u0026hellip;). Hands-on: Understand the practical steps to create an S3 bucket, host a static website, and configure AWS Backup. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DATA SCIENCE WORKSHOP ON AWS\nTime: 09:30, October 16, 2025\nLocation: FPT University, D1 Street, High-Tech Park, Tang Nhon Phu Ward, Ho Chi Minh City.\nRole in the event: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #1\nTime: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #2\nTime: 09:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 6 Event Name: AWS Cloud Mastery Series #3\nTime: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.4-test-chatbox/","title":"Testing Chatbot (RAG)","tags":[],"description":"","content":"Target After successfully ingesting data into the Vector Store, it is time to verify the results. In this section, you will act as an end-user, asking the Chatbot questions directly within the AWS Console interface to observe how the RAG system operates.\nWe will focus on 2 factors:\nAccuracy: Does the AI answer correctly based on the documents? Transparency: Can the AI cite the source (Citation) of the information? Implementation Steps Step 1: Configure the test window\nTo start chatting, we need to select a Foundation Model that will act as the \u0026ldquo;responder\u0026rdquo;.\nIn your Knowledge Base details interface, look at the right panel titled Test knowledge base. Click the Select model button.\nIn the selection panel that appears: Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet / Haiku depending on the model you enabled). Throughput: Keep On-demand. Click Apply. Step 2: Conduct conversation (Chat)\nNow, try asking a question related to the document content you uploaded.\nIn the input box (Message input), type your question. Example: If you uploaded the \u0026ldquo;AWS Overview\u0026rdquo; document, ask: \u0026ldquo;Can you explain to me what EC2 is?\u0026rdquo;. Click Run. Observe the result: The AI will think for a few seconds (querying the Vector Store). Then, it will answer in natural language, summarizing the found information. Step 3: Verify data source\nThis is the most important feature of RAG that distinguishes it from standard ChatGPT: the ability to prove the source of information.\nIn the AI\u0026rsquo;s response, pay attention to the small numbers (footnotes) or the text Show source details. Click on those numbers or the details button. A Source details window will appear, displaying: Source chunk: The exact original text segment that the AI found in the document. Score: Similarity score (relevance). S3 Location: Path to the original file. Seeing this original text segment proves that the AI is not \u0026ldquo;hallucinating\u0026rdquo; but is actually reading your documents.\nStep 4: Test with irrelevant questions (Optional)\nTo see how the system reacts when information is not found.\nAsk a question completely unrelated to the documents. Example: \u0026ldquo;Can you explain some knowledge about personal finance?\u0026rdquo; (While your documents are about Cloud Computing). Expected Result: The AI might answer based on its general knowledge (if not restricted). OR the AI will answer \u0026ldquo;Sorry, I am unable to answer your question based on the retrieved data\u0026rdquo; - This is the ideal behavior for an enterprise RAG application. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"AWS Cloud Mastery Series #2: From DevOps, IaC to Container \u0026amp; Observability Event Purpose Mindset Transformation: Deep understanding of the Value Cycle and how DevOps culture helps enterprises balance development speed and system stability. Modern Infrastructure Revolution: Transform from risky manual management (ClickOps) to Infrastructure as Code (IaC) with three tools: CloudFormation, Terraform, and CDK. Container Strategy: Analyze architecture and make decisions to choose the most suitable orchestration platform: From simple (App Runner), deep integration (ECS) to flexible scaling (EKS). Deep Observability: Establish proactive monitoring systems, not just for error reporting but also to understand system behavior and optimize user experience with CloudWatch and X-Ray. Speaker List AWS \u0026amp; Cloud Engineers Team: Bringing overall architectural perspective, Platform Engineering strategy, and practical demos. Mr. Tran Vi: FCJer 2024 - Sharing practical experience from the community. Mr. Long Quy Nghiem: FCJer 2024 - Sharing perspectives from someone newly approaching and developing Cloud skills. Detailed Content 1. DevOps Mindset \u0026amp; CI/CD Pipeline (Foundation of Thinking) The event emphasized that DevOps is not a job title or tool, but a philosophy focused on optimizing value flow from idea to end user.\nThe Value Cycle:\nA closed cycle consisting of 5 stages: Insights \u0026amp; Analysis -\u0026gt; Portfolio \u0026amp; Backlog (Planning) -\u0026gt; Continuous Integration -\u0026gt; Continuous Testing -\u0026gt; Continuous Delivery. Core Objective: Solve the classic \u0026ldquo;Speed vs. Stability\u0026rdquo; problem. DevOps proves we can increase the speed of launching new features (Speed) without sacrificing system safety (Stability) through automation. Redefining CI/CD concepts:\nContinuous Integration (CI): A culture of frequent code commits (daily). Every code change triggers automatic Build and Test processes to detect errors immediately (Fail fast), avoiding technical debt accumulation. Continuous Delivery: After passing CI, code is automatically deployed to Staging environment. However, deploying to Production is a business decision requiring human confirmation (Manual Approval). Continuous Deployment: The highest level of automation. If code passes all tests, it goes straight to Production without any human intervention. Effective Pipeline Strategy:\nCentralized CI: Platform team builds standard pipelines, ensuring security and compliance, but empowers Developers to self-service to avoid bottlenecks. Artifact Management: Follow the immutable principle \u0026ldquo;Build Once, Deploy Anywhere\u0026rdquo;. Source code is packaged only once into Binary/Docker Image (Artifact). Test, Staging, and Prod environments all use this same Artifact to ensure what was tested is what will run. Fail Fast Mechanism: Pipeline must be designed to stop immediately when there\u0026rsquo;s a problem (Compilation error, Bad code, Security vulnerability, Slow test). Better to stop the process early than let errors slip through to more expensive later stages. Effectiveness Metrics:\nUse Heatmap to visualize entire organization\u0026rsquo;s performance. Focus on 4 golden metrics (DORA Metrics): Deployment frequency, Lead time for changes, Change Failure Rate, and Mean Time To Recovery (MTTR). 2. Infrastructure as Code (IaC) - From ClickOps to Code This section analyzed the mandatory shift from manual management to infrastructure automation.\nThe Problem of \u0026ldquo;ClickOps\u0026rdquo;: Clicking on AWS Console is intuitive but poses major risks: Human errors (forgetting configuration), inability to recreate identical environments (Inconsistent), and extremely difficult when scaling. IaC Solution: Turn infrastructure into Code to enjoy software development benefits: Version Control (Git), Code Review, Testing, and Reusability. Detailed analysis of 3 leading IaC tools:\n1. AWS CloudFormation (Native Tool):\nAWS\u0026rsquo;s \u0026ldquo;official\u0026rdquo; tool, using YAML/JSON to declare desired state (Declarative). Template Anatomy: Structure includes Parameters (Flexible inputs), Mappings (Value mapping by region/environment), and Resources (Specific AWS resources). Stack Management: Manage resources in groups (Stack). When deleting Stack, all related resources are cleaned up, avoiding resource waste. 2. Terraform (Multi-Cloud Powerhouse):\nOpen-source tool using HCL language. The #1 choice for Multi-cloud strategy. Safe Process: Write -\u0026gt; Plan -\u0026gt; Apply. The Plan step allows previewing how changes will impact actual infrastructure before applying, helping avoid catastrophic mistakes. State File: Terraform\u0026rsquo;s \u0026ldquo;memory\u0026rdquo;, storing actual infrastructure state for comparison and synchronization. 3. AWS CDK (Cloud Development Kit):\nApproach infrastructure with modern programming languages (Python, TS, Java\u0026hellip;), leveraging loops, conditions, and object-oriented programming. Power of Abstraction (Constructs): L1: Raw configuration (CloudFormation equivalent). L2: Pre-built Classes with safe default configurations (Best practices). L3: Design Patterns building complex systems (VPC + Cluster + LB) with just a few lines of code. Drift Detection: Feature that detects \u0026ldquo;configuration drift\u0026rdquo; - the difference between Code (IaC) and reality (manual edits). This is an important tool for maintaining operational discipline.\n3. Containerization - Application Running Strategy Deep dive into orchestration models to choose optimal solutions:\nKubernetes (K8s):\nThe world\u0026rsquo;s Container standard. Complex architecture including Control Plane (brain) and Worker Nodes (muscle). Suitable for extremely large systems needing deep customization, but requires highly skilled operations team. Comparing Amazon ECS vs. Amazon EKS:\nAmazon ECS: \u0026ldquo;Simplified\u0026rdquo;. Designed by AWS to integrate seamlessly with other services (ALB, IAM). Suitable for teams wanting to focus on applications, less worry about cluster operations. Amazon EKS: \u0026ldquo;Open standard\u0026rdquo;. AWS\u0026rsquo;s Managed Kubernetes version. Suitable for enterprises needing K8s tool ecosystem or running Hybrid-cloud. Compute Options:\nEC2 Launch Type: You manage virtual machines (Servers). Maximum control but must handle OS patching, agent updates. AWS Fargate (Serverless): You only manage Containers. AWS handles all underlying server infrastructure. Eliminates OS maintenance burden. AWS App Runner:\n\u0026ldquo;Zero-ops\u0026rdquo; solution. For Developers wanting to deploy Web Apps/APIs as quickly as possible. Automates everything from Source Code -\u0026gt; Build -\u0026gt; Deploy -\u0026gt; Load Balancer -\u0026gt; HTTPS URL. 4. Observability - Monitoring \u0026amp; Optimization Shift from \u0026ldquo;Monitoring\u0026rdquo; (Is the system alive?) to \u0026ldquo;Observability\u0026rdquo; (Why is the system slow?).\nAmazon CloudWatch (Monitoring Center):\nMetrics: Quantitative measurements (High CPU, Full RAM). Logs: Detailed activity logs. Logs Insights helps query and analyze millions of log lines in seconds. Alarms: Automatic reaction mechanism. When metrics exceed threshold -\u0026gt; Send alert or Auto-scale system. AWS X-Ray (Distributed Tracing):\nSolves the \u0026ldquo;black box\u0026rdquo; problem in Microservices. Distributed Tracing: Maps the journey of a request through dozens of different services. Helps precisely identify which service is causing slowness (Latency) or errors to fix at the root. AWS Observability Best Practices:\nReference AWS Observability Recipes to apply standard monitoring patterns. Clearly distinguish roles: Logs provide event details, Traces provide context and flow of those events. Detailed Experience in the Event The specialized session completely changed how I view software system operations:\n1. The Shift from \u0026ldquo;Ops\u0026rdquo; to \u0026ldquo;Platform Engineering\u0026rdquo; I realized the role of modern DevOps is not \u0026ldquo;server attendant\u0026rdquo; or \u0026ldquo;hired deployer\u0026rdquo;. DevOps are Platform Builders. The mission is to create a safe and automated \u0026ldquo;highway\u0026rdquo; (Pipeline \u0026amp; Infrastructure), helping Developers bring code to market themselves (Self-service) without waiting, while still ensuring safety rules.\n2. Operational Discipline The concept of Immutability in Artifact management and Drift Detection in IaC is truly valuable. In enterprise environments, \u0026ldquo;it works\u0026rdquo; is not enough, it must be \u0026ldquo;stable and consistent\u0026rdquo;. Prohibiting manual edits (ClickOps) and following the \u0026ldquo;Code -\u0026gt; Build -\u0026gt; Deploy\u0026rdquo; process is vital to avoid silly human errors.\n3. Smart Tool Selection Strategy The biggest lesson is there\u0026rsquo;s no \u0026ldquo;best\u0026rdquo; tool, only \u0026ldquo;most suitable for context\u0026rdquo; tool:\nNeed absolute stability and latest AWS feature support? Choose CloudFormation. Enterprise using multiple Clouds (Multicloud)? Choose Terraform. Team strong in programming, want to write less code but get large infrastructure? Choose AWS CDK. Want to run simple Web App without managing K8s? App Runner is perfect. Conclusion The \u0026ldquo;DevOps \u0026amp; IaC Mastery\u0026rdquo; session painted a technology maturity roadmap:\nOn Mindset: Shift from intuitive, manual work to systematic thinking, automation, and data-driven measurement. On Infrastructure: Control infrastructure with Code (IaC) to achieve flexibility and unlimited scalability. On Operations: Combine Container power with system understanding capability (Observability) to ensure services are always available and optimized. This is the solid knowledge foundation for me to confidently step into building large-scale systems on AWS.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn foundational knowledge and core security services on AWS, centered around the \u0026ldquo;Security is job zero\u0026rdquo; philosophy. Start with the most basic concept: the Shared Responsibility Model. Focus deeply on managing identity and access (Identity and Access Management - IAM), including components: User, Group, Policy, and Role. Expand learning to identity management services at a larger scale, such as AWS Organizations (managing multiple accounts), AWS Identity Center (SSO) (single sign-on), and Amazon Cognito (user management for web/mobile apps). Gain solid knowledge of data protection through encryption with AWS KMS and monitoring/compliance checks with AWS Security Hub. Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Shared Responsibility Model: Security of the Cloud vs. Security in the Cloud.\nIAM:\n- Root Account protection.\n- User, Group, Policy types (Identity vs. Resource).\n- IAM Roles \u0026amp; Assume Role mechanism (STS). 06/10/2025 06/10/2025 Module 05 Tue Amazon Cognito: User Pools (Authentication) \u0026amp; Identity Pools (Authorization).\nAWS Organizations: Multi-account management, OU, SCPs.\nAWS Identity Center (SSO): Centralized access \u0026amp; Permission Sets. 07/10/2025 07/10/2025 Module 05 Wed AWS KMS: Encryption key management (CMK, Data Keys).\nAWS Security Hub: Automated security checks \u0026amp; scoring.\nLab 000002: IAM Basics \u0026amp; Role Assumption.\nLab 000044: IAM Role Conditions \u0026amp; Restrictions. 08/10/2025 08/10/2025 Module 05 Thu Lab 000048: IAM Roles for Applications (EC2).\nLab 000030: Implementing IAM Permission Boundaries.\nLab 000027 \u0026amp; 000028: Tagging strategies \u0026amp; Resource Groups management. 09/10/2025 09/10/2025 Module 05 Fri Lab 000018: AWS Security Hub setup \u0026amp; standards check.\nLab 000012: Configuring AWS SSO with Organizations.\nLab 000033: KMS Workshop (Encryption, Key Policies).\nExtra Study: AWS Security Specialty Guide. 10/10/2025 10/10/2025 Module 05 Research Link Week 5 Achievements: Foundational Lesson: Master the Shared Responsibility Model, clearly understanding AWS\u0026rsquo;s responsibilities versus the customer\u0026rsquo;s. IAM Service (Core): Clearly distinguish between the Root Account (full permissions, needs to be locked away) and IAM User (used daily, no permissions by default). Master the 3 main components for granting permissions: IAM User (the entity), IAM Policy (the permission - written in JSON), and IAM Group (a group of entities). Clearly understand IAM Role: a mechanism to grant temporary permissions (no permanent credentials) to both Users and Services (like EC2). IAM Techniques (Important): Know how a User/Service \u0026ldquo;receives\u0026rdquo; a Role\u0026rsquo;s permissions through the Assume Role technique (using the STS service). Understand the permission evaluation rule: An Explicit Deny always overrides any Allow permissions. Identity Management Services (Identity Services): Clearly differentiate between IAM (manages AWS administrators) and Amazon Cognito (manages end-users of web/mobile apps). Know that Cognito User Pool is the user directory (can log in with Facebook, Google) and Identity Pool is what grants those users access to AWS resources. Multi-Account Management Service (Multi-Account): Understand AWS Organizations is used for centrally managing multiple accounts, enabling Consolidated Billing. Know how to use Service Control Policies (SCP) within an Organization to limit the maximum permissions of member accounts. Understand AWS Identity Center (SSO) as the single sign-on solution, using Permission Sets to grant access to accounts within the Organization. Encryption Service (Encryption): Know AWS KMS is the service for creating and managing encryption keys. Understand the Encryption at Rest mechanism and differentiate between CMK (the master key in KMS) and Data Key (the key used to encrypt the actual data). Security Monitoring Service (Monitoring): Know AWS Security Hub is the service that scans and provides security scores, helping to check compliance against standards (like PCIDSS). Hands-on: Practice creating and managing Users, Groups, Policies, and Roles. Practice implementing SSO and KMS. Practice using advanced IAM features like Conditions and Permission Boundaries. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.5-client-integration/","title":"Client Application Integration (Optional)","tags":[],"description":"","content":"Target You will turn Python code into a professional Web Chatbot GUI (Graphical User Interface) that is user-friendly (similar to the ChatGPT interface) in just a few minutes.\nWe use:\nBackend: Python. Frontend: Streamlit. AI Model: Claude 3.5 Sonnet. Implementation Steps Part I: Configure AWS Credentials\nStep 1: Install AWS CLI\nOpen Terminal on your computer.\n# macOS brew install awscli # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Step 2: Configure credentials\naws configure Enter the information when prompted:\nAWS Access Key ID: YOUR_ACCESS_KEY AWS Secret Access Key: YOUR_SECRET_KEY Default region name: us-east-1 Default output format: json Step 3: Verify configuration\n# Check credentials aws sts get-caller-identity # Check Bedrock connection aws bedrock-agent-runtime list-knowledge-bases --region ap-southeast-1 Security notes:\nDO NOT commit credentials to Git DO NOT share credentials with others Use IAM roles when possible Rotate credentials periodically Required permissions:\nIAM User needs the following permissions:\nbedrock:InvokeModel bedrock:RetrieveAndGenerate bedrock:Retrieve s3:GetObject (for Knowledge Base) Troubleshooting:\nError \u0026ldquo;Unable to locate credentials\u0026rdquo;:\nCheck if ~/.aws/credentials file exists Check file format is correct Try running aws configure again Error \u0026ldquo;AccessDeniedException\u0026rdquo;:\nCheck IAM permissions Ensure region is correct (ap-southeast-1) Check Knowledge Base ID is correct Error \u0026ldquo;ExpiredToken\u0026rdquo;:\nCredentials have expired Need to create new credentials from AWS Console Part II: Clone Project from pre-made GitHub\nStep 1: Access the following GitHub link\nPlease download and open the folder above using Visual Studio Code:\nhttps://github.com/DazielNguyen/chatbot_with_bedrock.git\nStep 2: Install libraries and Python environment\nInstall environment:\nMacOS: python3 -m venv .venv Win: python -m venv .venv Activate environment:\nMacOS: source .venv/bin/activate Win: .venv\\Scripts\\activate Install libraries:\nMacOS/ Win: pip install -r requirements.txt Step 3: Get the ID of the created Knowledge Base\nAccess Amazon Bedrock -\u0026gt; Knowledge Base -\u0026gt; knowledge-base-demo Update \u0026ldquo;KB_ID=\u0026ldquo;YOUR_KNOWLEDGE_BASE_ID\u0026rdquo;\u0026rdquo; Step 4: Run Streamlit - Chatbot UI and Experience\nRun Terminal: streamlit run start.py When the command finishes running, the following page will appear: Try asking some questions you uploaded to the Knowledge Base earlier. The Chatbot has returned results based on the data file you provided, with citations of your data sources. Conclusion Congratulations on successfully building a Web Chatbot built with Amazon Bedrock\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a RAG Application using Knowledge Bases for Amazon Bedrock Overview Knowledge Bases for Amazon Bedrock is a fully managed feature that helps you implement RAG (Retrieval-Augmented Generation) techniques by connecting Foundation Models to your internal data sources to deliver accurate, cited, and contextually relevant responses.\nRAG is a technique to optimize Large Language Model (LLM) output by retrieving information from a trusted external database (Retrieval) and adding it to the context (Augmentation) before generating the answer (Generation). This method helps overcome limitations regarding outdated training data and ensures the AI answers based on the actual provided information.\nIn this lab, we will learn how to build an AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; proprietary enterprise documents. You will perform the process from data ingestion and creating vector indexes to configuring the model to answer questions based on those documents without managing any servers.\nWe will use three main components to set up a complete RAG processing workflow:\nData Source (Amazon S3) - Acts as the repository of \u0026ldquo;truth\u0026rdquo;. You will upload documents (PDF, Word, Text) to an S3 bucket. The Knowledge Base will use this source to synchronize data. Vector Store (OpenSearch Serverless) - The place to store vector embeddings (numerically encoded data). When a user asks a question, the system will perform a semantic search here to extract the most relevant text segments instead of standard keyword searching. Foundation Model (Claude 3) - The Large Language Model acting as the processing brain. It receives the user\u0026rsquo;s question along with information found from the Vector Store, then synthesizes and generates a natural, accurate answer accompanied by source citations. Outcomes By the end of the workshop, you will have a practical, functioning Chatbot system with the following features:\nQ\u0026amp;A chat regarding proprietary document content. Accurate answers, no hallucinations. Source citations (knowing exactly which page the answer comes from). Rapid deployment without writing complex data processing code. Contents Workshop Overview Environment Preparation Create and Configure Knowledge Base Test Chatbot (RAG) Client Application Integration (Optional) Update Data Clean Up Resources "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 Purpose of the Series The event went beyond tool demonstrations and focused on building System Thinking. The goal was to help participants shift from a passive security mindset to a proactive, comprehensive Cloud-Native Security model:\nCommunity Building: Create a sustainable network to share knowledge through AWS Cloud Clubs. Governance at Scale: Establish management foundations for large organizations (hundreds of accounts) to ensure consistency and compliance. Defense in Depth: Multi-layered security strategy (Identity - Network - Data) so the system remains safe even if one layer is breached. Automated Response: Move from manual incident handling to instant automated processes to minimize damage. Speakers The program brought together experienced experts and key figures in the AWS technical community in Vietnam:\nAWS Cloud Clubs representatives: Captains from major universities (HCMUTE, SGU, PTIT, HUFLIT). Identity \u0026amp; Governance experts: Huynh Hoang Long, Dinh Le Hoang Anh (AWS Community Builders). Detection \u0026amp; Monitoring experts: Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat. Network Security expert: Kha Van (Cloud Security Engineer | AWS Community Builder). Data Protection experts: Thinh Lam, Viet Nguyen. Incident Response experts: Mendel Grabski (Long) — former Head of Security \u0026amp; DevOps, and Tinh Truong — Platform Engineer. Detailed Content PART 1: KICKOFF - AWS CLOUD CLUBS \u0026amp; GROWTH OPPORTUNITIES Introduction to the AWS Cloud Clubs ecosystem — a launchpad for future Cloud talent.\nVision: More than a learning space, it is an environment to practice leadership and connect with the global Cloud community. Core Benefits: Build Skills: Hands-on projects, certification sponsorship, and focused learning accounts. Build Community: Shorten the gap between students and industry experts. Build Opportunities: Improve CV, receive AWS Credits to prototype ideas, and network for jobs. The Badging Journey: A gamified progression path to motivate participation. Levels: Bronze \u0026gt; Silver \u0026gt; Gold \u0026gt; Platinum \u0026gt; Diamond. Value: Recognition, swag, Credits ($200+), and priority access to major events like Student Community Day. PART 2: IDENTITY \u0026amp; GOVERNANCE On the Cloud, the network perimeter is no longer the only defense — Identity is the new perimeter.\nModern IAM mindset: Identity First: Treat identity as the primary defense. Credential Spectrum: Move away from long-term credentials (static access keys) to short-term credentials (STS tokens). Least Privilege: Grant only necessary permissions; avoid admin(*) for convenience. Governance at scale with AWS Organizations: Layered architecture: Organize accounts by function (OUs) — Security (logs, audit), Shared Services, Workloads — to isolate risk. Service Control Policies (SCPs): Guardrails (like a constitution) that enforce restrictions even on account root users. PART 3: VISIBILITY \u0026amp; DETECTION \u0026ldquo;You cannot protect what you cannot see.\u0026rdquo;\nAmazon GuardDuty: Intelligent detection using AI/ML over three core telemetry sources: CloudTrail (who did what?), VPC Flow Logs (where is traffic going?), and DNS logs (which sites are being accessed?). Runtime Monitoring: Optional lightweight agents to monitor OS-level behavior and detect suspicious processes. AWS Security Hub: Consolidates findings into ASFF (AWS Security Finding Format). Acts as a CSPM (Cloud Security Posture Management) to continuously check compliance (CIS, PCI-DSS) and report misconfigurations. PART 4: NETWORK SECURITY Build a digital fortress with a Zero Trust approach.\nVPC Fundamentals: Security Groups (stateful): Apply micro-segmentation; prefer reference-based rules over brittle IP whitelists. NACLs (stateless): Coarse-grained subnet-level deny lists to block malicious IP ranges. Advanced Filtering: Route 53 Resolver DNS Firewall: Block malware from reaching command-and-control servers. AWS Network Firewall: Deep Packet Inspection with both stateless (fast) and stateful engines (Suricata rules) for complex detections and FQDN filtering. Modern network architecture: Use AWS Transit Gateway to connect multiple VPCs and centralize inspection with Network Firewall. Active Threat Defense: Automate detection-to-block workflows (e.g., GuardDuty finds a malicious IP → update Network Firewall rules). PART 5: DATA PROTECTION Protect core assets with layered encryption.\nEnvelope Encryption: Use AWS KMS to encrypt data keys with a master key; data keys encrypt large objects to balance security and performance. Secrets Management: Avoid hardcoding credentials. Use AWS Secrets Manager for secure storage and automatic rotation of database credentials. Hardware-backed encryption: AWS Nitro offloads crypto operations to dedicated hardware (Nitro Cards) to provide full-disk cryptographic protections with minimal performance impact. PART 6: INCIDENT RESPONSE When defenses fail, speed matters.\nPrevention: Reduce attack surface: avoid long-lived SSH keys, block S3 public access at the account level, use private subnets. IaC: Security starts in code — use Terraform/CDK to review and control configuration changes. The 5-step playbook: Preparation: Tools, logs, and playbooks must be ready. Detection: Use CloudTrail and GuardDuty signals. Containment: Isolate compromised resources (e.g., attach quarantine Security Group via Lambda). Eradication \u0026amp; Recovery: Remove root cause and restore from clean backups. Post-Incident: Conduct RCA and improve defenses. Automation: Use EventBridge to trigger Lambda automations (for example, auto-close a public S3 bucket immediately when detected). Conclusion \u0026ldquo;Cloud Security \u0026amp; Operations Mastery\u0026rdquo; is a comprehensive guide for building resilient Cloud systems:\nIdentity \u0026amp; Governance: The foundation to control who can do what. Network \u0026amp; Detection: The sensors and barriers to observe and block threats. Data \u0026amp; Response: The vault and the rapid response team to mitigate incidents. Event Photos Photo: Over 400 attendees at AWS Cloud Mastery Series #3\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Review foundational database (DB) concepts, including RDBMS (primary key, foreign key), optimization techniques (Index, Partition), and operational concepts (Database Log, Buffer). Clearly differentiate between the two main DB system types: OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing or Data Warehouse). Understand the managed relational database service Amazon RDS, including core features like Multi-AZ (for high availability) and Read Replicas (for read performance). Learn about Amazon Aurora, AWS\u0026rsquo;s cloud-native DB service, with its unique shared storage architecture, high performance, and superior features like Zero Replication Lag. Learn about Amazon Redshift, the petabyte-scale Data Warehouse service designed for OLAP, and understand its MPP architecture and Columnar Storage technique. Understand the role of Amazon ElastiCache (Redis, Memcached) as a high-speed caching layer to reduce load on the primary database. Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Database Concepts:\n- RDBMS architecture (PK, FK, Normalization) \u0026amp; Optimization (Index, Partition, Execution Plan).\n- RDBMS (ACID) vs. NoSQL (BASE).\n- Workloads: OLTP (Transactional) vs. OLAP (Analytical). 13/10/2025 13/10/2025 Module 06 Tue Amazon RDS:\n- Managed service features, Automated Backups.\n- Multi-AZ: Synchronous replication for High Availability.\n- Read Replicas: Asynchronous replication for Read Scaling. 14/10/2025 14/10/2025 Module 06 Wed Amazon Aurora:\n- Storage Architecture: Shared Volume, 6 copies across 3 AZs, Zero Lag.\n- Cluster Structure: 1 Writer + up to 15 Readers.\n- Features: Backtrack, Global Database. 15/10/2025 15/10/2025 Module 06 Thu Amazon Redshift: Data Warehouse, MPP architecture, Columnar Storage for OLAP, Redshift Spectrum.\nAmazon ElastiCache: In-memory caching (Redis/Memcached) to offload primary DB. 16/10/2025 16/10/2025 Module 06 Fri Lab 000005: Amazon RDS Basics (Create, Connect, Backup/Restore).\nLab 000043: DB Migration using DMS \u0026amp; SCT (Oracle to Aurora).\nExtra Study: Database Internals \u0026amp; The Data Warehouse Toolkit books. 17/10/2025 17/10/2025 Module 06 Week 6 Achievements: Lesson (Foundational): Clearly differentiate between the two system models: OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing, data warehouse). Master basic DB optimization techniques: Index (speeds up reads) and Partition (divides tables). Understand the role of Database Log (for recovery/replication) and Buffer (uses RAM for speed). Service (RDS): Know Amazon RDS is a managed relational database (OLTP) service. Clearly distinguish RDS\u0026rsquo;s 2 main features: Multi-AZ (used for High Availability - HA) and Read Replicas (used to increase read performance). Technique (Replication): Differentiate Synchronous Replication (used for RDS Multi-AZ) and Asynchronous Replication (used for RDS Read Replicas, can have lag). Service (Aurora): Know Amazon Aurora is a high-performance, cloud-native database. Understand Aurora\u0026rsquo;s shared storage (Cluster Volume) architecture and its superior benefit of Zero Replication Lag. Be aware of advanced features like Backtrack and Global Database. Service (Redshift): Know Amazon Redshift is a data warehouse (OLAP) service. Understand the MPP (Massively Parallel Processing) architecture (includes Leader Node and Compute Nodes). Master the core technique of OLAP: Columnar Storage, which speeds up analytical queries. Service (ElastiCache): Know Amazon ElastiCache (Redis/Memcached) is an in-RAM caching service. Understand the role of caching is to reduce load on the primary DB. Be aware of the responsibility to write the Caching Logic in the application. Hands-on: Know how to create and operate (backup/restore) an RDS database. Know how to use DMS and SCT services to migrate a DB from Oracle to Aurora. "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS/First cloud journey program from 12/08/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in in learning AWS services, as well as applying learned knowledge into a group project with a FinTech theme, through which I improved my skills in data collection, data analysis, reporting and communication.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.6-update-data/","title":"Update data","tags":[],"description":"","content":"Target One of the biggest advantages of RAG compared to Fine-tuning (retraining) a model is the ability to update data quickly. When a business has new regulations, you simply need to ingest them into the Knowledge Base, and the AI will \u0026ldquo;learn\u0026rdquo; them immediately.\nIn this section, we will simulate the following scenario:\nAsk the AI for a piece of non-existent information (The AI will answer that it doesn\u0026rsquo;t know). Provide that information to the system by uploading a new file. Ask the same question again to witness the AI answer correctly immediately. Implementation Steps Step 1: Verify the initial \u0026ldquo;lack of knowledge\u0026rdquo;\nWe need to confirm that the current AI knows nothing about the confidential information we are about to create.\nReturn to the Streamlit Chatbot interface (created in Part 5) or use the Test Knowledge Base window on the Console. Ask a question about hypothetical fake information. Example: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Observe the result: The AI will answer that it cannot find the information in the provided documents or will attempt a generic answer (if not restricted). Step 2: Create new data\nWe will create a text file containing this \u0026ldquo;secret\u0026rdquo; to ingest into the system.\nOn your computer, open Notepad (Windows) or TextEdit (Mac). Copy and paste the following content into the file: CONFIDENTIAL NOTICE: The secret Project Omega officially launches on 01/12/2025. The activation code is: \u0026#34;AWS-ROCKS-2025-SINGAPORE\u0026#34;. The Project Manager is Mr. Robot. Please keep this information strictly confidential. Save the file as: secret-project.txt. You can download the file here: TXT format file\nStep 3: Upload and Sync\nNow, we will feed this new knowledge into the AI\u0026rsquo;s \u0026ldquo;brain\u0026rdquo;.\nAccess the S3 Console, navigate to your old bucket (rag-workshop-demo).\nClick Upload -\u0026gt; Add files -\u0026gt; Select the secret-project.txt file -\u0026gt; Upload.\nSwitch to the Amazon Bedrock Console -\u0026gt; Select Knowledge bases from the left menu. Click on your Knowledge Base name. Scroll down to the Data source section, select the data source (s3-datasource). Click the Sync button (Orange). Wait: Wait for about 30 seconds to 1 minute until the Status column changes from Syncing to Available. Step 4: Verify again (The \u0026ldquo;Wow\u0026rdquo; Moment)\nThe system now possesses the new knowledge. Let\u0026rsquo;s challenge the AI once again.\nReturn to the Streamlit Chatbot interface (No need to reload the page or restart the server). Ask the exact same question as before: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Expected Result: The AI answers correctly: \u0026ldquo;The activation code is AWS-ROCKS-2025-SINGAPORE\u0026rdquo;. The AI cites the source as the secret-project.txt file. Conclusion You have just witnessed the true power of RAG!\nNo code editing required. No model retraining required. Simply Sync the data. Your Chatbot has become smarter and updated with the latest information in just a few simple steps. This is exactly why businesses choose this solution to build internal virtual assistants.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn and research more about Cloud knowledge Complete MOOCs in Skill Builder to review for Cloud Practitioner Certificate Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Domain 1: Cloud Concepts\n- 5 Cloud Computing criteria.\n- Availability vs. Reliability vs. Resiliency.\n- Scaling vs. Elasticity.\n- AWS Well-Architected Framework (6 pillars). 20/10/2025 20/10/2025 Tue Domain 1: Cloud Adoption \u0026amp; Economics\n- AWS Cloud Adoption Framework (CAF) \u0026amp; Stages.\n- Migration Strategies (The 7 R\u0026rsquo;s) \u0026amp; Scenarios.\n- Cloud Economics, Pricing \u0026amp; Total Cost of Ownership (TCO). 21/10/2025 21/10/2025 Wed Domain 2: Security \u0026amp; Compliance\n- Shared Responsibility Model.\n- Access Management: Root, IAM, Cognito, Policies.\n- Security Components: Security Groups, NACL, WAF. 22/10/2025 22/10/2025 Thu Domain 3: Technology \u0026amp; Services (Part 1)\n- Global Infra: Regions, AZs, Edge (CloudFront vs. Global Accelerator).\n- Compute: EC2 families, Containers, Serverless computing.\n- Database: RDS, Aurora, DynamoDB, Redshift \u0026amp; DB Migration. 23/10/2025 23/10/2025 Fri Domain 3: Technology \u0026amp; Services (Part 2)\n- Networking: VPC architecture, Hybrid Connectivity, DNS.\n- Storage: Object (S3), Block (EBS), File (EFS), Backup.\n- Other Services: AI/ML, Analytics, Monitoring, App Integration, DevOps, IoT. 24/10/2025 24/10/2025 Week 7 Achievements: 1. Domain 1: Cloud Concepts - Benefits of AWS Cloud Cloud Computing Basics:\nMastered 5 characteristics of cloud computing (On-demand self-service, Broad network access, Resource pooling, Rapid elasticity, Measured service) Understood AWS cloud infrastructure clearly Distinguished concepts clearly: Availability: System is always operational and accessible Durability: Data is protected from loss Resilience: System can recover quickly after incidents Understood the difference between Scaling and Elasticity AWS Well-Architected Framework:\nMastered General Design Principles Understood clearly the 6 main pillars: Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability Migration to AWS Cloud:\nUnderstood AWS Cloud Adoption Framework (CAF) with 6 perspectives Mastered Cloud Adoption Stages Learned 7 migration strategies (7 R\u0026rsquo;s): Rehost (Lift and Shift) Replatform (Lift, Tinker, and Shift) Repurchase (Drop and Shop) Refactor/Re-architect Retire Retain Relocate Understood migration services and scenarios Cloud Economics:\nUnderstood Total Cost of Ownership (TCO) Cost reduction methods on AWS AWS billing and pricing models 2. Domain 2: Security and Compliance AWS Shared Responsibility Model:\nAWS responsibility (Security OF the Cloud) Customer responsibility (Security IN the Cloud) Responsibility changes based on service type (IaaS, PaaS, SaaS) Security, Governance \u0026amp; Compliance:\nAWS basic security concepts Governance and compliance on AWS Cloud Security services and tools Access Management:\nAWS Root User and best practices AWS IAM (Identity and Access Management) Amazon Cognito IAM Policies and how they work Least Privilege Principle Security Components:\nSecurity Groups: Instance-level firewall Network ACLs (NACLs): Subnet-level firewall AWS WAF (Web Application Firewall): Web application protection 3. Domain 3: Cloud Technology and Services (Part 1) 3.1 Deployment and Operating Methods:\nDeployment and operation methods Cloud types: Public, Private, Hybrid Compare Public vs Private services Connectivity options 3.2 AWS Global Infrastructure:\nAWS global infrastructure structure (Regions, Availability Zones) Regional expansion utilities (Local Zones, Wavelength Zones) Edge Services: CloudFront vs Global Accelerator Concepts of Availability Zones and Disaster Recovery 3.3 AWS Compute Resources:\nAmazon EC2: Instance types, AMI, configuration EC2 Storage: EBS, Instance Store Containers: ECS, EKS, Fargate Serverless Computing: AWS Lambda Compare High Availability vs Scalability 3.4 AWS Database Resources:\nAmazon RDS: Managed relational database Amazon Aurora: MySQL/PostgreSQL compatible Amazon DynamoDB: NoSQL database In-Memory Databases: ElastiCache, DynamoDB Accelerator (DAX) Amazon Redshift: Data warehouse Migration Services: AWS Snow Family, AWS DMS, AWS DataSync 4. Domain 3: Cloud Technology and Services (Part 2) 3.5 AWS Network Resources:\nAmazon VPC and core components: Subnets (Public/Private) Route Tables Internet Gateway NAT Gateway VPC Security: Security Groups, NACLs VPC Gateways: Internet Gateway, NAT Gateway, Virtual Private Gateway VPC \u0026amp; Hybrid Connectivity: VPN, Direct Connect DNS \u0026amp; Routing: Route 53 3.6 AWS Storage Resources:\nCloud storage types: Object, Block, File Amazon S3: Object storage, storage classes File Storage: EFS vs FSx Block Storage: EBS vs Instance Store EBS Volume Types: gp2, gp3, io1, io2, st1, sc1 AWS Storage Gateway: Hybrid storage Backup Solutions: AWS Backup 3.7 AI/ML and Analytics Services:\nAI/ML basics 3 levels of AWS ML: AI Services (Rekognition, Comprehend, etc.) ML Services (SageMaker) ML Frameworks \u0026amp; Infrastructure Analytics Services: Amazon Athena: Query S3 data Amazon Macie: Data security Amazon Redshift: Data warehouse Amazon Kinesis: Real-time data streaming Amazon Glue: ETL service Amazon QuickSight: BI tool Amazon EMR: Big data processing 3.8 Other AWS Services:\nMonitoring \u0026amp; Observability: CloudWatch, X-Ray, EventBridge Application Integration: SQS, SNS Business \u0026amp; Customer Services: Amazon Connect, SES, AWS Activate, AWS IQ Developer \u0026amp; DevOps Tools: CodeCommit, CodeBuild, CodeDeploy, CodePipeline End-User Computing: AppStream 2.0, WorkSpaces, WorkSpaces Web Frontend \u0026amp; Mobile: AWS Amplify, AWS AppSync IoT Services: AWS IoT Core, AWS IoT Greengrass Summary: Week 7 completed comprehensive learning of 3 main domains for AWS Cloud Practitioner certificate. Mastered basic cloud computing concepts, understood security and compliance on AWS clearly, and became familiar with most important AWS services. Built a solid knowledge foundation to prepare for the certification exam.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/5-workshop/5.7-cleanup/","title":"Clean Resources","tags":[],"description":"","content":"Target To avoid incurring unwanted costs after finishing the practice lab, we need to delete the created resources.\n⚠️ WARNING: Deleting the Knowledge Base DOES NOT automatically delete the Vector Store (OpenSearch Serverless). You must manually delete the OpenSearch Serverless Collection as this is the costliest service in this Lab.\nImplementation Steps Step 1: Delete Knowledge Base\nAccess the Amazon Bedrock Console -\u0026gt; Knowledge bases.\nSelect the radio button next to your Knowledge Base name.\nClick the Delete button.\nA dialog box appears, enter the Knowledge Base name to confirm (or type delete).\nClick Delete. This process takes 10-15 minutes to complete successfully, so please be patient.\nStep 2: Delete Vector Store\nAccess the Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. You will see a Collection named like bedrock-knowledge-base-.... Select the radio button next to that Collection name. Click the Delete button. Type confirm or the collection name to confirm deletion. Click Delete. Step 3: Delete Data on S3\nAccess the Amazon S3 service. Select the bucket rag-workshop-demo. Click the Empty button first. Type permanently delete to confirm deleting all files inside. After the bucket is empty, return to the Buckets list. Select that bucket again and click the Delete button. Enter the bucket name to confirm. Completion Congratulations on fully completing the Workshop \u0026ldquo;Building a RAG Application with Amazon Bedrock\u0026rdquo;. Your system has been cleaned up and is safe!\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What I liked most was being able to go to the office, sit in the air-conditioned room, and work. What do you think the company should improve for future interns? It\u0026rsquo;s easy to go out to use the restroom, but getting back in can be difficult — sometimes the door is locked from the outside. If recommending to a friend, would you suggest they intern here? Why or why not? I would definitely recommend it to my friends, because not only is the working environment very professional, but it\u0026rsquo;s also convenient for going out in District 1. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? I have no ideas yet. Would you like to continue this program in the future? Yes, of course I would. I still want to challenge myself with this internship. Any other comments (free sharing): "},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Complete courses on NLP (Natural Language Processing) Learn more about FastAPI to prepare for the upcoming project Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Module 01: NLP Classification \u0026amp; Vector Spaces\n- Sentiment Analysis (Logistic Regression, Naive Bayes).\n- Vector Space Models, Machine Translation \u0026amp; Doc Search. 27/10/2025 27/10/2025 Tue Module 02: Probabilistic Models\n- Autocorrect \u0026amp; POS Tagging (Hidden Markov Models).\n- Autocomplete, Language Models \u0026amp; Word Embeddings. 28/10/2025 28/10/2025 Wed Module 03: Sequence Models\n- RNNs, LSTMs, and Named Entity Recognition (NER).\n- Siamese Networks. 29/10/2025 29/10/2025 Thu Module 04: Attention Models\n- Neural Machine Translation (NMT).\n- Text Summarization \u0026amp; Question Answering (QA). 30/10/2025 30/10/2025 Fri Deployment \u0026amp; API:\n- Deploying ML models as APIs.\n- Building a complete CRUD application using FastAPI. 31/10/2025 31/10/2025 Week 8 Achievements: 1. Module 01 - Natural Language Processing with Classification and Vector Spaces Week 01: Sentiment Analysis with Logistic Regression\nUnderstood binary classification for sentiment analysis Built logistic regression model from scratch Feature extraction with Bag of Words Preprocessing: tokenization, stemming, stop words removal Model evaluation with accuracy, precision, recall Week 02: Sentiment Analysis with Naive Bayes\nUnderstood Bayes\u0026rsquo; Theorem and conditional probability Built Naive Bayes classifier Compared performance with Logistic Regression Understood independence assumption Laplacian smoothing to handle zero probability Week 03: Vector Space Models\nVector space models and word representations Cosine similarity to measure similarity PCA (Principal Component Analysis) for dimensionality reduction Visualized word embeddings Euclidean distance vs Cosine similarity Week 04: Machine Translation and Document Search\nWord alignment for machine translation Hash tables and locality sensitive hashing Document search and information retrieval K-nearest neighbors in NLP Transformation matrices for word translation 2. Module 02 - Natural Language Processing with Probabilistic Models Week 01: Auto-correction and Minimum Edit Distance\nEdit distance (Levenshtein distance) Dynamic programming for minimum edit distance Spelling correction algorithms Probability-based error correction N-gram models for spell checking Week 02: Part-of-Speech Tagging and Hidden Markov Models\nPart-of-Speech (POS) tagging Hidden Markov Models (HMMs) Viterbi algorithm for sequence labeling Transition and emission probabilities Training HMMs with tagged corpus Week 03: Autocomplete and Language Models\nN-gram language models (unigram, bigram, trigram) Perplexity for evaluating language models Smoothing techniques (Laplace, Add-k) Backoff and interpolation Building autocomplete systems Week 04: Word Embeddings with Neural Networks\nContinuous Bag of Words (CBOW) Skip-gram model Word2Vec architecture Training word embeddings Negative sampling Evaluating word embeddings 3. Module 03 - Natural Language Processing with Sequence Models Week 01: Recurrent Neural Networks for Language Modeling\nRNN architecture and forward propagation Backpropagation through time (BPTT) Vanishing and exploding gradient problems Language modeling with RNNs Text generation with RNNs GRU (Gated Recurrent Units) Week 02: LSTMs and Named Entity Recognition\nLong Short-Term Memory (LSTM) architecture Cell state, forget gate, input gate, output gate Named Entity Recognition (NER) task Bidirectional LSTMs Training LSTMs for sequence labeling Evaluating NER systems Week 03: Siamese Networks\nSiamese network architecture Triplet loss function One-shot learning Similarity learning Applications: question duplicate detection, semantic similarity Cosine similarity in neural networks 4. Module 04 - Natural Language Processing with Attention Models Week 01: Neural Machine Translation\nSequence-to-sequence (Seq2Seq) models Encoder-decoder architecture Attention mechanism Teacher forcing BLEU score for machine translation Beam search decoding Week 02: Text Summarization\nExtractive vs Abstractive summarization Seq2Seq with attention for summarization ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) Coverage mechanism Pointer-generator networks Handling long documents Week 03: Question Answering\nQuestion answering systems Context-based QA Attention mechanisms for QA SQuAD dataset Extractive QA models End-to-end trainable QA systems 5. FastAPI and Machine Learning Model Deployment Machine Learning Model as API:\nSetup FastAPI project structure Load and serve ML models Request/Response schemas with Pydantic Input validation and error handling Preprocessing pipelines in API Testing API endpoints Dockerize ML API FastAPI CRUD Application:\nRESTful API design principles CRUD operations (Create, Read, Update, Delete) Database integration (SQLAlchemy) Async/await operations Authentication and authorization API documentation with Swagger/OpenAPI Dependency injection in FastAPI File structure best practices: app/main.py: Entry point app/routers/: API routes app/models/: Database models app/schemas/: Pydantic schemas app/crud/: Database operations app/db/: Database configuration FastAPI Key Features Mastered:\nPath parameters and query parameters Request body validation Response models Background tasks Middleware CORS configuration Environment variables Testing with pytest Summary: Week 8 completed the entire NLP curriculum from basic to advanced, including 4 modules covering topics from classification, probabilistic models, sequence models to attention mechanisms. Mastered techniques from traditional methods (Naive Bayes, HMM) to modern deep learning approaches (RNN, LSTM, Attention). Also mastered FastAPI framework to deploy ML models as production-ready APIs with full CRUD operations, validation, and best practices. Ready to apply NLP and FastAPI knowledge to real-world projects.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: FastAPI project with MongoDB Vietnamese STT library selected and integrated Vietnamese OCR library selected and tested NLP extraction: amount, category, date, jar detection (REQ-027) Detect multiple transactions (REQ-027) Publish events to RabbitMQ Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Project Setup: FastAPI structure, Docker, MongoDB setup.\nTech Research: Research optimal Vietnamese OCR (Bills) and STT (Voice) models. 03/11/2025 03/11/2025 Tue Core API Development:\n- Setup Endpoints, Pydantic models.\n- Configure Middleware (CORS, Error handling) \u0026amp; Logging. 04/11/2025 04/11/2025 Wed Voice \u0026amp; OCR Implementation:\n- Voice: Install STT libs, audio pre-processing.\n- OCR: Install libs, research image pre-processing techniques. 05/11/2025 05/11/2025 Thu Optimization:\n- Voice: Implement PhoWhisper (VinAI), transaction/timestamp detection.\n- OCR: Enhance image quality, collect Vietnamese bill datasets. 06/11/2025 06/11/2025 Fri Integration \u0026amp; Testing:\n- Handle background tasks.\n- E2E Testing for Voice flow.\n- Evaluate OCR models (Tesseract, EasyOCR, etc.). 07/11/2025 07/11/2025 Week 9 Achievements: 1. Project Setup and Infrastructure Completed FastAPI project structure with standard directories (/app, /models, /services, /utils, /routers, /schemas, /ai-models) Setup Python 3.11+ environment with virtual environment Installed and configured local MongoDB using Docker Created ai_service_db database and collections for Bills and Voices Setup MongoDB connection with MongoEngine and lifecycle management 2. API Structure and Middleware Setup API endpoints for Voice and Bill processing Created Pydantic models for request/response validation Configured CORS middleware and error handling middleware Implemented health check endpoint (GET /health) Integrated structlog for logging system 3. Voice Processing (Speech-to-Text) Researched and selected PhoWhisper from VinAI for Vietnamese STT Implemented audio preprocessing and Voice-to-Text integration Tested model accuracy and voice detection capability Configured endpoint to return correct defined categories Tested processing multiple transactions simultaneously Setup time detection for transactions from speech Created transaction objects from voice data 4. Bill Processing (OCR) Researched and selected appropriate OCR libraries (Tesseract, EasyOCR) Implemented image preprocessing to optimize quality before OCR Collected diverse Vietnamese bill dataset (electricity, supermarket, restaurants, convenience stores, coffee shops) Tested OCR models with real bill images 5. NLP Extraction and Data Processing Implemented information extraction: amount, category, date (REQ-027) Jar detection Multiple transaction detection in a single request 6. RabbitMQ Integration Setup event publishing to RabbitMQ Handled background tasks for Voice and Bill processing 7. Testing and Quality Assurance End-to-end testing for Voice processing pipeline (Recording → Processing → Return Endpoint) Evaluated and improved Voice processing accuracy Tested OCR models with real dataset Summary: Week 9 successfully completed all objectives, including FastAPI-MongoDB infrastructure setup, PhoWhisper model integration for Vietnamese STT, OCR implementation for bill processing, and event publishing system with RabbitMQ. NLP functions for transaction information extraction were implemented and successfully tested.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Improve and optimize OCR model for various types of invoices Enhance Voice processing quality with Vietnamese numbers and compound phrases Deploy Confidence Scoring system for both Voice and Bill Smart category detection and context recognition Comprehensive error handling and performance optimization Multi-dimensional testing with edge cases Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Cloud Mastery Series #2.\nOCR Enhancement: Extract line items to JSON, item counting.\nVoice Enhancement: Vietnamese number normalization, noise reduction, and spelling correction. 10/11/2025 10/11/2025 Tue Confidence Scoring Implementation:\n- OCR: Field-level scoring (amount, date), low confidence threshold.\n- Voice: Multi-layer scoring, performance testing (\u0026lt;4s).\n- System-wide testing. 11/11/2025 11/11/2025 Wed Advanced Features \u0026amp; Integration:\n- Voice: Smart categorization \u0026amp; context detection.\n- OCR: Support new bill types (supermarkets, restaurants, cafes).\n- Backend: Transaction integration, standardize JSON output. 12/11/2025 12/11/2025 Thu Error Handling \u0026amp; Optimization:\n- Handle corrupted files, timeouts, and Retry logic.\n- Edge Cases: Silence, non-Vietnamese audio, large files.\n- Graceful Degradation: Return partial results instead of full failure. 13/11/2025 13/11/2025 Fri Comprehensive Testing \u0026amp; Documentation:\n- API Testing for Voice/Bill.\n- Stress Testing: Voice (background noise, speed), Bill (rotated, blurry, complex images).\n- Finalize documentation. 14/11/2025 14/11/2025 Week 10 Achievements: 1. OCR Model Improvements Detect and extract line items in proper JSON format Extract multiple items in a single invoice Automatically calculate quantity of components in invoice Successfully tested with various invoice types: supermarket, restaurant, coffee shop, beverage stores Built extraction rules for specific invoice types Improved JSON response format for Bill endpoint 2. Voice Processing Quality Enhancement Handle Vietnamese numbers (convert \u0026ldquo;hai mươi hai\u0026rdquo; → \u0026ldquo;22\u0026rdquo;) Recognize Vietnamese compound phrases Fix spelling errors and handle audio noise Improved JSON response format Optimized Voice processing performance 3. Confidence Scoring System OCR Confidence:\nCalculate field-level confidence Confidence for Amount/Total Confidence for Date Evaluate text clarity and line separation Detect price alignment and quantity Build overall confidence algorithm Set Low Confidence Threshold Tested on over 30 real invoices Voice Confidence:\nDeploy multi-layer confidence scoring Overall confidence algorithm for Voice Set Low Confidence Thresholds Performance check on over 50 samples (processing time \u0026lt;4 seconds) 4. Smart Category Detection Merchant/seller name analysis Keyword extraction based on categories Transaction context determination Advanced Category Detection 5. Error Handling \u0026amp; Resilience Audio Error Handling:\nValidate audio format (WAV, MP3, M4A) Check duration (min: 0.5s, max: 30s) Detect corrupted/incomplete files Handle STT timeout (max: 30s) Retry logic with exponential backoff (3 times, 1s-2s-4s) Exception Handling:\nEmpty/silent audio → Error: \u0026ldquo;No speech detected\u0026rdquo; Non-Vietnamese speech → Low confidence warning Multiple speakers → Warning + best effort extraction Audio too long (\u0026gt;30s) → Error: \u0026ldquo;Audio too long\u0026rdquo; Graceful Degradation:\nCategory not detected → Return \u0026ldquo;Uncategorized\u0026rdquo; Quantity not extracted → Return null + warning Date not detected → Use current date + warning Always return partial results when possible 6. Comprehensive Testing Voice Testing:\nTest with background noise Test speech speed (fast/slow) Test invalid input (nonsense speech) Test multiple transactions in one recording Test ambiguous input Bill OCR Testing:\nTest images rotated in various directions Test diverse image quality Test input not in invoice format Test highly complex invoices Test invoices with many items 7. Backend Integration Integrate transaction functions with Backend Comprehensive error handling Return standardized JSON values Summary: Week 10 focused on enhancing quality and reliability of both Voice and OCR models. Successfully deployed confidence scoring system, comprehensive error handling, and multi-dimensional testing with various edge cases. The system is ready for Backend integration and handling complex real-world scenarios.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Detect and handle multiple complex transactions Integrate PDF file support for Bill OCR Implement feedback system and model fine-tuning Enhance image and audio processing quality Integrate Backend and MongoDB storage Automated testing and Docker deployment Get acquainted with AWS and management tools Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Advanced Features:\n- Multi-transaction detection \u0026amp; smart jar assignment parsing.\n- Add support and testing for PDF bill processing. 17/11/2025 17/11/2025 Tue Feedback Loop:\n- Implement user feedback collection system for Voice/OCR.\n- Fine-tune models based on feedback data to improve accuracy. 18/11/2025 18/11/2025 Wed Advanced Processing:\n- Bill: Auto-rotation, de-skewing, ROI detection, image quality enhancement.\n- Voice: Background noise reduction, Voice Activity Detection (VAD), silence trimming. 19/11/2025 19/11/2025 Thu System Integration:\n- Integrate Backend APIs, verify automated transaction flow.\n- Setup MongoDB storage and prepare for AWS S3 integration. 20/11/2025 20/11/2025 Fri Testing \u0026amp; Deployment:\n- Setup Automated Testing and Regression testing.\n- Containerize and deploy application to Docker. 21/11/2025 21/11/2025 Week 11 Achievements: 1. Multiple Transaction Detection and PDF Support Complex multi-transaction parsing Refined multi-transaction detection algorithm Detected jar allocation in phrases Handled mixed transaction types Complex Phrase Testing:\nTested 1 transaction with 1 jar Tested multiple transactions Tested switching between jars Tested mixed contexts PDF Support:\nInstalled PDF processing libraries Tested transactions with PDF files Integrated PDF into OCR Pipeline 2. Feedback and Learning System Handled user feedback system for Voice section Handled user feedback system for Bill section Fine-tuned models based on feedback Improved handling of incorrect input syntax Tested improvements of Voice and Bill models 3. Advanced Image/Audio Processing Bill Quality Enhancement:\nDetected and assessed image quality Auto-rotated and deskewed images Detected ROI (Region of Interest) Integrated with OCR Pipeline Tested with various image conditions Voice Background Noise Handling:\nImplemented noise reduction Voice Activity Detection Silence trimming Reduced processing time Integrated with Voice Pipeline Tested with various audio environments 4. Backend \u0026amp; Storage Integration Backend Integration:\nReviewed Backend API endpoints Tested AI and Backend workflow Verified event consumption Checked automatic transaction creation Troubleshot integration issues MongoDB Integration:\nSetup Database for Voice and Bill Prepared schema for Amazon S3 integration Implemented storage strategy 5. Automated Testing and Deployment Created automated tests for Voice and Bill Acceptance and verification of all test cases Fixed bugs discovered from testing Performed full regression testing Setup Docker environment Deployed to Docker container 6. AWS Learning Understood AWS and basic service groups (Compute, Storage, Networking, Database) Created and configured AWS Free Tier account Became familiar with AWS Management Console Installed and configured AWS CLI (Access Key, Secret Key, Region) Performed basic operations with AWS CLI Connected and became familiar with First Cloud Journey community Summary: Week 11 completed the enhancement of multi-transaction processing capabilities, integrated PDF support, implemented feedback system and model fine-tuning. Significantly improved image/audio processing quality with advanced techniques (ROI detection, noise reduction, VAD), successfully integrated with Backend and MongoDB, while completing automated testing and Docker deployment. Additionally, became familiar with AWS ecosystem and basic management tools, preparing for cloud infrastructure deployment.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Load testing and system performance optimization Improve accuracy of Voice and OCR models Enhance security and comprehensive error handling Implement advanced logging and metrics collection Prepare for deployment and final testing Improve code quality and documentation Tasks to be carried out this week: Day Task Start Date End Date Resources \u0026amp; Notes Mon Load Testing: Setup \u0026amp; execute scenarios (Voice/Bill/Concurrent).\nOptimization: Enhance Database performance, Caching strategy, and Memory management. 24/11/2025 24/11/2025 Tue Accuracy Improvement:\n- Voice/OCR: Analyze failures, refine NLP rules \u0026amp; character recognition.\n- Edge Cases: Handle quantity ambiguity and total amount validation. 25/11/2025 25/11/2025 Wed Security \u0026amp; Robustness:\n- Security: Rate limiting, Input validation, JWT, MongoDB hardening.\n- Robustness: Comprehensive Error Handling \u0026amp; Testing with corrupted inputs. 26/11/2025 26/11/2025 Thu Observability:\n- Configure Logging (Structured JSON, Stack traces).\n- Collect Metrics (Processing time, Error rates).\n- Prepare for Deployment. 27/11/2025 27/11/2025 Fri Final QA: Full Regression \u0026amp; Integration Testing (UI/Backend).\nCode Quality: Add Docstrings, Type hints, run Linters, and write Unit Tests. 28/11/2025 28/11/2025 Week 12 Achievements: 1. Load Testing and Optimization Load Testing Setup:\nInstalled load testing tools (JMeter/Locust) Created load testing scenarios (Voice, Bill, concurrent) Setup resource monitoring (CPU, RAM, Disk I/O) Prepared test data Running Load Tests:\nTested Voice load (10 files concurrently) Tested Bill OCR load (10 files concurrently) Tested concurrent load for both Voice and Bill Analyzed bottlenecks and chokepoints Optimization:\nOptimized Database queries and indexing Implemented caching for results Optimized memory and garbage collection Improved API response time 2. Accuracy Improvements Voice Accuracy:\nAnalyzed failure cases Improved NLP rules for Vietnamese Tested and iterated Enhanced accuracy for number and category recognition OCR Accuracy:\nAnalyzed OCR failure cases Format-specific improvements for bills Improved special character recognition Handled difficult font cases Amount Parsing:\nHandled ambiguous cases Validation logic for amounts Total amount extraction Total validation logic 3. Security Enhancements File Security:\nFile upload validation (file type, size validation) Rate limiting for APIs Input sanitization JWT token validation MongoDB security (authentication, authorization) Completed security checklist Error Handling:\nComprehensive Try-Catch for all functions Appropriate HTTP status codes Clear and helpful error messages Logging with full context Robustness Testing:\nTested Voice with corrupted/invalid files Tested OCR with corrupted/invalid images Handled graceful degradation 4. Logging and Metrics Enhanced Logging:\nStructured JSON logging Logging for each HTTP request Processing step logs with timestamps Error logging with stack traces Correlation IDs for tracking request flow Metrics Collection:\nTracked processing time Accuracy and error rates Stored metrics in MongoDB Created API endpoints for metrics Dashboard for monitoring 5. Deployment Preparation Prepared Voice service deployment Prepared OCR service deployment Docker configuration and optimization Environment variables and secrets management Health check endpoints 6. Comprehensive Testing and Code Quality Comprehensive Testing:\nFull regression testing Tested all error scenarios UI integration testing (frontend integration) Backend integration testing End-to-end testing Code Quality:\nAdded docstrings for all functions/classes Added type hints (Python typing) Ran Linter (Pylint/Flake8) and fixed issues Added unit tests for critical functions Code review and refactoring Summary: Week 12 focused on finalizing and making the AI system production-ready. Successfully performed load testing and performance optimization, significantly improved accuracy of both Voice and OCR models. Implemented comprehensive security with file validation, rate limiting, JWT authentication, and MongoDB security. Deployed structured logging and metrics collection for monitoring. Enhanced code quality with docstrings, type hints, linting, and unit tests. The system is now ready for production deployment with robust error handling and comprehensive testing.\n"},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://pgmdes.github.io/fcj2025-workshop-minhtran/tags/","title":"Tags","tags":[],"description":"","content":""}]